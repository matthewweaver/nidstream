{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1b611aa",
   "metadata": {},
   "source": [
    "# Feature Engineering for Network Intrusion Detection\n",
    "\n",
    "This notebook performs feature engineering on the BCCC-CSE-CIC-IDS2018 dataset.\n",
    "\n",
    "## Objectives:\n",
    "1. Load and preprocess raw network flow data\n",
    "2. Handle missing values and outliers\n",
    "3. Create derived features\n",
    "4. Encode categorical variables\n",
    "5. Scale numerical features\n",
    "6. Handle class imbalance\n",
    "7. Save processed features for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e7f6d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22f5cc4",
   "metadata": {},
   "source": [
    "## 1. Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4124e2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 289,799 records\n",
      "Number of features: 323\n",
      "\n",
      "Columns: ['flow_id', 'timestamp', 'src_ip', 'src_port', 'dst_ip', 'dst_port', 'protocol', 'duration', 'packets_count', 'fwd_packets_count', 'bwd_packets_count', 'total_payload_bytes', 'fwd_total_payload_bytes', 'bwd_total_payload_bytes', 'payload_bytes_max', 'payload_bytes_min', 'payload_bytes_mean', 'payload_bytes_std', 'payload_bytes_variance', 'payload_bytes_median', 'payload_bytes_skewness', 'payload_bytes_cov', 'payload_bytes_mode', 'fwd_payload_bytes_max', 'fwd_payload_bytes_min', 'fwd_payload_bytes_mean', 'fwd_payload_bytes_std', 'fwd_payload_bytes_variance', 'fwd_payload_bytes_median', 'fwd_payload_bytes_skewness', 'fwd_payload_bytes_cov', 'fwd_payload_bytes_mode', 'bwd_payload_bytes_max', 'bwd_payload_bytes_min', 'bwd_payload_bytes_mean', 'bwd_payload_bytes_std', 'bwd_payload_bytes_variance', 'bwd_payload_bytes_median', 'bwd_payload_bytes_skewness', 'bwd_payload_bytes_cov', 'bwd_payload_bytes_mode', 'total_header_bytes', 'max_header_bytes', 'min_header_bytes', 'mean_header_bytes', 'std_header_bytes', 'median_header_bytes', 'skewness_header_bytes', 'cov_header_bytes', 'mode_header_bytes', 'variance_header_bytes', 'fwd_total_header_bytes', 'fwd_max_header_bytes', 'fwd_min_header_bytes', 'fwd_mean_header_bytes', 'fwd_std_header_bytes', 'fwd_median_header_bytes', 'fwd_skewness_header_bytes', 'fwd_cov_header_bytes', 'fwd_mode_header_bytes', 'fwd_variance_header_bytes', 'bwd_total_header_bytes', 'bwd_max_header_bytes', 'bwd_min_header_bytes', 'bwd_mean_header_bytes', 'bwd_std_header_bytes', 'bwd_median_header_bytes', 'bwd_skewness_header_bytes', 'bwd_cov_header_bytes', 'bwd_mode_header_bytes', 'bwd_variance_header_bytes', 'fwd_avg_segment_size', 'bwd_avg_segment_size', 'avg_segment_size', 'fwd_init_win_bytes', 'bwd_init_win_bytes', 'active_min', 'active_max', 'active_mean', 'active_std', 'active_median', 'active_skewness', 'active_cov', 'active_mode', 'active_variance', 'idle_min', 'idle_max', 'idle_mean', 'idle_std', 'idle_median', 'idle_skewness', 'idle_cov', 'idle_mode', 'idle_variance', 'bytes_rate', 'fwd_bytes_rate', 'bwd_bytes_rate', 'packets_rate', 'bwd_packets_rate', 'fwd_packets_rate', 'down_up_rate', 'avg_fwd_bytes_per_bulk', 'avg_fwd_packets_per_bulk', 'avg_fwd_bulk_rate', 'avg_bwd_bytes_per_bulk', 'avg_bwd_packets_bulk_rate', 'avg_bwd_bulk_rate', 'fwd_bulk_state_count', 'fwd_bulk_total_size', 'fwd_bulk_per_packet', 'fwd_bulk_duration', 'bwd_bulk_state_count', 'bwd_bulk_total_size', 'bwd_bulk_per_packet', 'bwd_bulk_duration', 'fin_flag_counts', 'psh_flag_counts', 'urg_flag_counts', 'ece_flag_counts', 'syn_flag_counts', 'ack_flag_counts', 'cwr_flag_counts', 'rst_flag_counts', 'fwd_fin_flag_counts', 'fwd_psh_flag_counts', 'fwd_urg_flag_counts', 'fwd_ece_flag_counts', 'fwd_syn_flag_counts', 'fwd_ack_flag_counts', 'fwd_cwr_flag_counts', 'fwd_rst_flag_counts', 'bwd_fin_flag_counts', 'bwd_psh_flag_counts', 'bwd_urg_flag_counts', 'bwd_ece_flag_counts', 'bwd_syn_flag_counts', 'bwd_ack_flag_counts', 'bwd_cwr_flag_counts', 'bwd_rst_flag_counts', 'fin_flag_percentage_in_total', 'psh_flag_percentage_in_total', 'urg_flag_percentage_in_total', 'ece_flag_percentage_in_total', 'syn_flag_percentage_in_total', 'ack_flag_percentage_in_total', 'cwr_flag_percentage_in_total', 'rst_flag_percentage_in_total', 'fwd_fin_flag_percentage_in_total', 'fwd_psh_flag_percentage_in_total', 'fwd_urg_flag_percentage_in_total', 'fwd_ece_flag_percentage_in_total', 'fwd_syn_flag_percentage_in_total', 'fwd_ack_flag_percentage_in_total', 'fwd_cwr_flag_percentage_in_total', 'fwd_rst_flag_percentage_in_total', 'bwd_fin_flag_percentage_in_total', 'bwd_psh_flag_percentage_in_total', 'bwd_urg_flag_percentage_in_total', 'bwd_ece_flag_percentage_in_total', 'bwd_syn_flag_percentage_in_total', 'bwd_ack_flag_percentage_in_total', 'bwd_cwr_flag_percentage_in_total', 'bwd_rst_flag_percentage_in_total', 'fwd_fin_flag_percentage_in_fwd_packets', 'fwd_psh_flag_percentage_in_fwd_packets', 'fwd_urg_flag_percentage_in_fwd_packets', 'fwd_ece_flag_percentage_in_fwd_packets', 'fwd_syn_flag_percentage_in_fwd_packets', 'fwd_ack_flag_percentage_in_fwd_packets', 'fwd_cwr_flag_percentage_in_fwd_packets', 'fwd_rst_flag_percentage_in_fwd_packets', 'bwd_fin_flag_percentage_in_bwd_packets', 'bwd_psh_flag_percentage_in_bwd_packets', 'bwd_urg_flag_percentage_in_bwd_packets', 'bwd_ece_flag_percentage_in_bwd_packets', 'bwd_syn_flag_percentage_in_bwd_packets', 'bwd_ack_flag_percentage_in_bwd_packets', 'bwd_cwr_flag_percentage_in_bwd_packets', 'bwd_rst_flag_percentage_in_bwd_packets', 'packets_IAT_mean', 'packet_IAT_std', 'packet_IAT_max', 'packet_IAT_min', 'packet_IAT_total', 'packets_IAT_median', 'packets_IAT_skewness', 'packets_IAT_cov', 'packets_IAT_mode', 'packets_IAT_variance', 'fwd_packets_IAT_mean', 'fwd_packets_IAT_std', 'fwd_packets_IAT_max', 'fwd_packets_IAT_min', 'fwd_packets_IAT_total', 'fwd_packets_IAT_median', 'fwd_packets_IAT_skewness', 'fwd_packets_IAT_cov', 'fwd_packets_IAT_mode', 'fwd_packets_IAT_variance', 'bwd_packets_IAT_mean', 'bwd_packets_IAT_std', 'bwd_packets_IAT_max', 'bwd_packets_IAT_min', 'bwd_packets_IAT_total', 'bwd_packets_IAT_median', 'bwd_packets_IAT_skewness', 'bwd_packets_IAT_cov', 'bwd_packets_IAT_mode', 'bwd_packets_IAT_variance', 'subflow_fwd_packets', 'subflow_bwd_packets', 'subflow_fwd_bytes', 'subflow_bwd_bytes', 'delta_start', 'handshake_duration', 'handshake_state', 'min_bwd_packets_delta_time', 'max_bwd_packets_delta_time', 'mean_packets_delta_time', 'mode_packets_delta_time', 'variance_packets_delta_time', 'std_packets_delta_time', 'median_packets_delta_time', 'skewness_packets_delta_time', 'cov_packets_delta_time', 'mean_bwd_packets_delta_time', 'mode_bwd_packets_delta_time', 'variance_bwd_packets_delta_time', 'std_bwd_packets_delta_time', 'median_bwd_packets_delta_time', 'skewness_bwd_packets_delta_time', 'cov_bwd_packets_delta_time', 'min_fwd_packets_delta_time', 'max_fwd_packets_delta_time', 'mean_fwd_packets_delta_time', 'mode_fwd_packets_delta_time', 'variance_fwd_packets_delta_time', 'std_fwd_packets_delta_time', 'median_fwd_packets_delta_time', 'skewness_fwd_packets_delta_time', 'cov_fwd_packets_delta_time', 'min_packets_delta_len', 'max_packets_delta_len', 'mean_packets_delta_len', 'mode_packets_delta_len', 'variance_packets_delta_len', 'std_packets_delta_len', 'median_packets_delta_len', 'skewness_packets_delta_len', 'cov_packets_delta_len', 'min_bwd_packets_delta_len', 'max_bwd_packets_delta_len', 'mean_bwd_packets_delta_len', 'mode_bwd_packets_delta_len', 'variance_bwd_packets_delta_len', 'std_bwd_packets_delta_len', 'median_bwd_packets_delta_len', 'skewness_bwd_packets_delta_len', 'cov_bwd_packets_delta_len', 'min_fwd_packets_delta_len', 'max_fwd_packets_delta_len', 'mean_fwd_packets_delta_len', 'mode_fwd_packets_delta_len', 'variance_fwd_packets_delta_len', 'std_fwd_packets_delta_len', 'median_fwd_packets_delta_len', 'skewness_fwd_packets_delta_len', 'cov_fwd_packets_delta_len', 'min_header_bytes_delta_len', 'max_header_bytes_delta_len', 'mean_header_bytes_delta_len', 'mode_header_bytes_delta_len', 'variance_header_bytes_delta_len', 'std_header_bytes_delta_len', 'median_header_bytes_delta_len', 'skewness_header_bytes_delta_len', 'cov_header_bytes_delta_len', 'min_bwd_header_bytes_delta_len', 'max_bwd_header_bytes_delta_len', 'mean_bwd_header_bytes_delta_len', 'mode_bwd_header_bytes_delta_len', 'variance_bwd_header_bytes_delta_len', 'std_bwd_header_bytes_delta_len', 'median_bwd_header_bytes_delta_len', 'skewness_bwd_header_bytes_delta_len', 'cov_bwd_header_bytes_delta_len', 'min_fwd_header_bytes_delta_len', 'max_fwd_header_bytes_delta_len', 'mean_fwd_header_bytes_delta_len', 'mode_fwd_header_bytes_delta_len', 'variance_fwd_header_bytes_delta_len', 'std_fwd_header_bytes_delta_len', 'median_fwd_header_bytes_delta_len', 'skewness_fwd_header_bytes_delta_len', 'cov_fwd_header_bytes_delta_len', 'min_payload_bytes_delta_len', 'max_payload_bytes_delta_len', 'mean_payload_bytes_delta_len', 'mode_payload_bytes_delta_len', 'variance_payload_bytes_delta_len', 'std_payload_bytes_delta_len', 'median_payload_bytes_delta_len', 'skewness_payload_bytes_delta_len', 'cov_payload_bytes_delta_len', 'min_bwd_payload_bytes_delta_len', 'max_bwd_payload_bytes_delta_len', 'mean_bwd_payload_bytes_delta_len', 'mode_bwd_payload_bytes_delta_len', 'variance_bwd_payload_bytes_delta_len', 'std_bwd_payload_bytes_delta_len', 'median_bwd_payload_bytes_delta_len', 'skewness_bwd_payload_bytes_delta_len', 'cov_bwd_payload_bytes_delta_len', 'min_fwd_payload_bytes_delta_len', 'max_fwd_payload_bytes_delta_len', 'mean_fwd_payload_bytes_delta_len', 'mode_fwd_payload_bytes_delta_len', 'variance_fwd_payload_bytes_delta_len', 'std_fwd_payload_bytes_delta_len', 'median_fwd_payload_bytes_delta_len', 'skewness_fwd_payload_bytes_delta_len', 'cov_fwd_payload_bytes_delta_len', 'label']\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "project_root = Path().resolve()\n",
    "data_path = project_root / 'data' / 'raw' / 'friday_02_03_2018_combined_sample.csv'\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"Loaded {len(df):,} records\")\n",
    "print(f\"Number of features: {len(df.columns)}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aae62fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 289799 entries, 0 to 289798\n",
      "Columns: 323 entries, flow_id to label\n",
      "dtypes: float64(259), int64(56), object(8)\n",
      "memory usage: 714.2+ MB\n",
      "None\n",
      "\n",
      "Missing values:\n",
      "payload_bytes_cov                  108358\n",
      "fwd_payload_bytes_cov               75401\n",
      "bwd_payload_bytes_cov               59023\n",
      "bwd_packets_IAT_skewness            49868\n",
      "bwd_packets_IAT_cov                 49868\n",
      "fwd_packets_IAT_cov                 34082\n",
      "fwd_packets_IAT_skewness            34027\n",
      "cov_payload_bytes_delta_len         28349\n",
      "cov_fwd_payload_bytes_delta_len     18905\n",
      "cov_bwd_payload_bytes_delta_len     15945\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check data types and missing values\n",
    "print(\"Data Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2891990f",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc02c177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'label' as target variable\n",
      "\n",
      "Feature matrix shape: (289799, 322)\n",
      "Target shape: (289799,)\n",
      "\n",
      "Class distribution:\n",
      "label\n",
      "Benign    278864\n",
      "Bot        10935\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Feature matrix shape: (289799, 322)\n",
      "Target shape: (289799,)\n",
      "\n",
      "Class distribution:\n",
      "label\n",
      "Benign    278864\n",
      "Bot        10935\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "# Identify the label column (could be 'label', 'Label', etc.)\n",
    "label_col = None\n",
    "for col in ['label', 'Label', 'label_name']:\n",
    "    if col in df.columns:\n",
    "        label_col = col\n",
    "        break\n",
    "\n",
    "if label_col is None:\n",
    "    raise ValueError(\"No label column found in dataset\")\n",
    "\n",
    "print(f\"Using '{label_col}' as target variable\")\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop(columns=[label_col])\n",
    "y = df[label_col]\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nClass distribution:\\n{y.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa5dbfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling missing values...\n",
      "\n",
      "Columns with missing values:\n",
      "payload_bytes_cov                  108358\n",
      "fwd_payload_bytes_cov               75401\n",
      "bwd_payload_bytes_cov               59023\n",
      "packets_IAT_cov                       437\n",
      "fwd_packets_IAT_skewness            34027\n",
      "fwd_packets_IAT_cov                 34082\n",
      "bwd_packets_IAT_skewness            49868\n",
      "bwd_packets_IAT_cov                 49868\n",
      "cov_packets_delta_time                437\n",
      "cov_bwd_packets_delta_time              1\n",
      "cov_fwd_packets_delta_time             55\n",
      "cov_packets_delta_len                5696\n",
      "cov_bwd_packets_delta_len            1398\n",
      "cov_fwd_packets_delta_len            3102\n",
      "cov_header_bytes_delta_len           7780\n",
      "cov_bwd_header_bytes_delta_len       1464\n",
      "cov_fwd_header_bytes_delta_len       4140\n",
      "cov_payload_bytes_delta_len         28349\n",
      "cov_bwd_payload_bytes_delta_len     15945\n",
      "cov_fwd_payload_bytes_delta_len     18905\n",
      "dtype: int64\n",
      "\n",
      "Columns with missing values:\n",
      "payload_bytes_cov                  108358\n",
      "fwd_payload_bytes_cov               75401\n",
      "bwd_payload_bytes_cov               59023\n",
      "packets_IAT_cov                       437\n",
      "fwd_packets_IAT_skewness            34027\n",
      "fwd_packets_IAT_cov                 34082\n",
      "bwd_packets_IAT_skewness            49868\n",
      "bwd_packets_IAT_cov                 49868\n",
      "cov_packets_delta_time                437\n",
      "cov_bwd_packets_delta_time              1\n",
      "cov_fwd_packets_delta_time             55\n",
      "cov_packets_delta_len                5696\n",
      "cov_bwd_packets_delta_len            1398\n",
      "cov_fwd_packets_delta_len            3102\n",
      "cov_header_bytes_delta_len           7780\n",
      "cov_bwd_header_bytes_delta_len       1464\n",
      "cov_fwd_header_bytes_delta_len       4140\n",
      "cov_payload_bytes_delta_len         28349\n",
      "cov_bwd_payload_bytes_delta_len     15945\n",
      "cov_fwd_payload_bytes_delta_len     18905\n",
      "dtype: int64\n",
      "\n",
      "Remaining missing values: 0\n",
      "\n",
      "Final feature matrix shape: (289799, 322)\n",
      "\n",
      "Remaining missing values: 0\n",
      "\n",
      "Final feature matrix shape: (289799, 322)\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values\n",
    "print(\"Handling missing values...\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_counts = X.isnull().sum()\n",
    "missing_cols = missing_counts[missing_counts > 0]\n",
    "\n",
    "if len(missing_cols) > 0:\n",
    "    print(f\"\\nColumns with missing values:\\n{missing_cols}\")\n",
    "    \n",
    "    # Strategy: Fill numeric columns with median, drop columns with >50% missing\n",
    "    threshold = 0.5\n",
    "    high_missing = missing_cols[missing_cols / len(X) > threshold]\n",
    "    \n",
    "    if len(high_missing) > 0:\n",
    "        print(f\"\\nDropping columns with >{threshold*100}% missing: {list(high_missing.index)}\")\n",
    "        X = X.drop(columns=high_missing.index)\n",
    "    \n",
    "    # Fill remaining missing values with median for numeric columns\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if X[col].isnull().any():\n",
    "            X[col] = X[col].fillna(X[col].median())\n",
    "    \n",
    "    print(f\"\\nRemaining missing values: {X.isnull().sum().sum()}\")\n",
    "else:\n",
    "    print(\"No missing values found!\")\n",
    "\n",
    "print(f\"\\nFinal feature matrix shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2375d115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for infinite values...\n",
      "\n",
      "Replaced infinite values in 9 columns\n",
      "  cov_packets_delta_len: 998 infinite values\n",
      "  cov_bwd_packets_delta_len: 1093 infinite values\n",
      "  cov_fwd_packets_delta_len: 141 infinite values\n",
      "  cov_header_bytes_delta_len: 488 infinite values\n",
      "  cov_bwd_header_bytes_delta_len: 1102 infinite values\n",
      "  cov_fwd_header_bytes_delta_len: 63 infinite values\n",
      "  cov_payload_bytes_delta_len: 204435 infinite values\n",
      "  cov_bwd_payload_bytes_delta_len: 94260 infinite values\n",
      "  cov_fwd_payload_bytes_delta_len: 183487 infinite values\n",
      "\n",
      "Replaced infinite values in 9 columns\n",
      "  cov_packets_delta_len: 998 infinite values\n",
      "  cov_bwd_packets_delta_len: 1093 infinite values\n",
      "  cov_fwd_packets_delta_len: 141 infinite values\n",
      "  cov_header_bytes_delta_len: 488 infinite values\n",
      "  cov_bwd_header_bytes_delta_len: 1102 infinite values\n",
      "  cov_fwd_header_bytes_delta_len: 63 infinite values\n",
      "  cov_payload_bytes_delta_len: 204435 infinite values\n",
      "  cov_bwd_payload_bytes_delta_len: 94260 infinite values\n",
      "  cov_fwd_payload_bytes_delta_len: 183487 infinite values\n"
     ]
    }
   ],
   "source": [
    "# Handle infinite values\n",
    "print(\"Checking for infinite values...\")\n",
    "\n",
    "numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "inf_counts = {}\n",
    "\n",
    "for col in numeric_cols:\n",
    "    inf_count = np.isinf(X[col]).sum()\n",
    "    if inf_count > 0:\n",
    "        inf_counts[col] = inf_count\n",
    "        # Replace inf with NaN, then fill with column median\n",
    "        X[col] = X[col].replace([np.inf, -np.inf], np.nan)\n",
    "        X[col] = X[col].fillna(X[col].median())\n",
    "\n",
    "if inf_counts:\n",
    "    print(f\"\\nReplaced infinite values in {len(inf_counts)} columns\")\n",
    "    for col, count in list(inf_counts.items())[:10]:\n",
    "        print(f\"  {col}: {count} infinite values\")\n",
    "else:\n",
    "    print(\"No infinite values found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ade653e",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbc08614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 1: TEMPORAL FEATURE EXTRACTION\n",
      "================================================================================\n",
      "Converting timestamp to datetime...\n",
      "✓ Created temporal features:\n",
      "  - hour (0-23), day_of_week (0-6), day_of_month, month\n",
      "  - is_business_hours, is_night, is_weekend, is_late_night\n",
      "  - hour_sin/cos, day_of_week_sin/cos (cyclical encoding)\n",
      "  ✓ Dropped original timestamp column\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "STEP 2: PORT FEATURE ENGINEERING\n",
      "================================================================================\n",
      "Encoding destination port (target service)...\n",
      "✓ Created destination port features:\n",
      "  - Binary flags for common services: http, https, ssh, ftp, smtp, dns, etc.\n",
      "  - Port range categories: well_known, registered, ephemeral\n",
      "  ✓ Keeping original dst_port column (will be scaled later)\n",
      "\n",
      "Handling source port...\n",
      "✓ Created source port features:\n",
      "  - src_port_is_privileged (<1024)\n",
      "  - src_port_is_ephemeral (>=49152)\n",
      "  ✓ Keeping original src_port column (will be scaled later)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "STEP 3: PROTOCOL ENCODING\n",
      "================================================================================\n",
      "Protocol values: ['TCP']\n",
      "✓ Created protocol features:\n",
      "  - protocol_tcp, protocol_udp, protocol_icmp\n",
      "  ✓ Dropped original protocol column\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "STEP 4: DROP IDENTIFIER COLUMNS\n",
      "================================================================================\n",
      "Dropping identifier columns: ['flow_id', 'src_ip', 'dst_ip']\n",
      "✓ Created destination port features:\n",
      "  - Binary flags for common services: http, https, ssh, ftp, smtp, dns, etc.\n",
      "  - Port range categories: well_known, registered, ephemeral\n",
      "  ✓ Keeping original dst_port column (will be scaled later)\n",
      "\n",
      "Handling source port...\n",
      "✓ Created source port features:\n",
      "  - src_port_is_privileged (<1024)\n",
      "  - src_port_is_ephemeral (>=49152)\n",
      "  ✓ Keeping original src_port column (will be scaled later)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "STEP 3: PROTOCOL ENCODING\n",
      "================================================================================\n",
      "Protocol values: ['TCP']\n",
      "✓ Created protocol features:\n",
      "  - protocol_tcp, protocol_udp, protocol_icmp\n",
      "  ✓ Dropped original protocol column\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "STEP 4: DROP IDENTIFIER COLUMNS\n",
      "================================================================================\n",
      "Dropping identifier columns: ['flow_id', 'src_ip', 'dst_ip']\n",
      "\n",
      "⚠️  Dropping remaining non-numeric columns: ['delta_start', 'handshake_duration', 'dst_port_cat_ephemeral', 'dst_port_cat_registered', 'dst_port_cat_well_known']\n",
      "\n",
      "✓ Final feature count: 343\n",
      "\n",
      "⚠️  Dropping remaining non-numeric columns: ['delta_start', 'handshake_duration', 'dst_port_cat_ephemeral', 'dst_port_cat_registered', 'dst_port_cat_well_known']\n",
      "\n",
      "✓ Final feature count: 343\n",
      "✓ All features are numeric: True\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "FEATURE ENGINEERING SUMMARY\n",
      "================================================================================\n",
      "Total features: 343\n",
      "  - Temporal features: 11\n",
      "  - Port features: 15\n",
      "  - Protocol features: 3\n",
      "  - Original/derived features: 314\n",
      "================================================================================\n",
      "\n",
      "✓ All features are numeric: True\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "FEATURE ENGINEERING SUMMARY\n",
      "================================================================================\n",
      "Total features: 343\n",
      "  - Temporal features: 11\n",
      "  - Port features: 15\n",
      "  - Protocol features: 3\n",
      "  - Original/derived features: 314\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# IMPROVED FEATURE ENGINEERING\n",
    "# ============================================================\n",
    "# Issues Fixed:\n",
    "# 1. Port numbers treated as continuous (WRONG - they're categorical!)\n",
    "# 2. Timestamp dropped (WRONG - temporal patterns are valuable!)\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 1: TEMPORAL FEATURE EXTRACTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Extract temporal features BEFORE dropping timestamp\n",
    "if 'timestamp' in X.columns:\n",
    "    print(\"Converting timestamp to datetime...\")\n",
    "    X['timestamp'] = pd.to_datetime(X['timestamp'])\n",
    "    \n",
    "    # Basic time features\n",
    "    X['hour'] = X['timestamp'].dt.hour\n",
    "    X['day_of_week'] = X['timestamp'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "    X['day_of_month'] = X['timestamp'].dt.day\n",
    "    X['month'] = X['timestamp'].dt.month\n",
    "    \n",
    "    # Binary indicators for suspicious times\n",
    "    X['is_business_hours'] = ((X['hour'] >= 9) & (X['hour'] <= 17)).astype(int)\n",
    "    X['is_night'] = ((X['hour'] >= 0) & (X['hour'] <= 5)).astype(int)\n",
    "    X['is_weekend'] = (X['day_of_week'] >= 5).astype(int)\n",
    "    X['is_late_night'] = ((X['hour'] >= 22) | (X['hour'] <= 4)).astype(int)\n",
    "    \n",
    "    # Cyclical encoding (preserves continuity: 23:59 is close to 00:01)\n",
    "    X['hour_sin'] = np.sin(2 * np.pi * X['hour'] / 24)\n",
    "    X['hour_cos'] = np.cos(2 * np.pi * X['hour'] / 24)\n",
    "    X['day_of_week_sin'] = np.sin(2 * np.pi * X['day_of_week'] / 7)\n",
    "    X['day_of_week_cos'] = np.cos(2 * np.pi * X['day_of_week'] / 7)\n",
    "    \n",
    "    print(f\"✓ Created temporal features:\")\n",
    "    print(f\"  - hour (0-23), day_of_week (0-6), day_of_month, month\")\n",
    "    print(f\"  - is_business_hours, is_night, is_weekend, is_late_night\")\n",
    "    print(f\"  - hour_sin/cos, day_of_week_sin/cos (cyclical encoding)\")\n",
    "    \n",
    "    # NOW drop the original timestamp\n",
    "    X = X.drop(columns=['timestamp'])\n",
    "    print(f\"  ✓ Dropped original timestamp column\")\n",
    "else:\n",
    "    print(\"⚠️  No timestamp column found\")\n",
    "\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# ============================================================\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 2: PORT FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'dst_port' in X.columns:\n",
    "    print(\"Encoding destination port (target service)...\")\n",
    "    \n",
    "    # Well-known ports (these are the ones attackers often target)\n",
    "    X['dst_port_http'] = (X['dst_port'].isin([80, 8080, 8000, 8888])).astype(int)\n",
    "    X['dst_port_https'] = (X['dst_port'] == 443).astype(int)\n",
    "    X['dst_port_ssh'] = (X['dst_port'] == 22).astype(int)\n",
    "    X['dst_port_ftp'] = (X['dst_port'].isin([20, 21])).astype(int)\n",
    "    X['dst_port_smtp'] = (X['dst_port'].isin([25, 587, 465])).astype(int)\n",
    "    X['dst_port_dns'] = (X['dst_port'] == 53).astype(int)\n",
    "    X['dst_port_telnet'] = (X['dst_port'] == 23).astype(int)\n",
    "    X['dst_port_smb'] = (X['dst_port'].isin([139, 445])).astype(int)\n",
    "    X['dst_port_rdp'] = (X['dst_port'] == 3389).astype(int)\n",
    "    X['dst_port_mysql'] = (X['dst_port'] == 3306).astype(int)\n",
    "    X['dst_port_postgres'] = (X['dst_port'] == 5432).astype(int)\n",
    "    \n",
    "    # Port range categories\n",
    "    def categorize_dst_port(port):\n",
    "        if port < 1024:\n",
    "            return 'well_known'\n",
    "        elif port < 49152:\n",
    "            return 'registered'\n",
    "        else:\n",
    "            return 'ephemeral'\n",
    "    \n",
    "    X['dst_port_category'] = X['dst_port'].apply(categorize_dst_port)\n",
    "    \n",
    "    # One-hot encode the category\n",
    "    dst_port_dummies = pd.get_dummies(X['dst_port_category'], prefix='dst_port_cat')\n",
    "    X = pd.concat([X, dst_port_dummies], axis=1)\n",
    "    X = X.drop(columns=['dst_port_category'])\n",
    "    \n",
    "    print(f\"✓ Created destination port features:\")\n",
    "    print(f\"  - Binary flags for common services: http, https, ssh, ftp, smtp, dns, etc.\")\n",
    "    print(f\"  - Port range categories: well_known, registered, ephemeral\")\n",
    "    \n",
    "    # Keep original dst_port for model to learn additional patterns\n",
    "    print(f\"  ✓ Keeping original dst_port column (will be scaled later)\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️  No dst_port column found\")\n",
    "\n",
    "if 'src_port' in X.columns:\n",
    "    print(\"\\nHandling source port...\")\n",
    "    \n",
    "    # Source port is usually ephemeral (random), less predictive\n",
    "    # But we can create some useful features\n",
    "    X['src_port_is_privileged'] = (X['src_port'] < 1024).astype(int)\n",
    "    X['src_port_is_ephemeral'] = (X['src_port'] >= 49152).astype(int)\n",
    "    \n",
    "    print(f\"✓ Created source port features:\")\n",
    "    print(f\"  - src_port_is_privileged (<1024)\")\n",
    "    print(f\"  - src_port_is_ephemeral (>=49152)\")\n",
    "    print(f\"  ✓ Keeping original src_port column (will be scaled later)\")\n",
    "\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# ============================================================\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 3: PROTOCOL ENCODING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'protocol' in X.columns:\n",
    "    print(f\"Protocol values: {X['protocol'].unique()}\")\n",
    "    \n",
    "    # Common protocols\n",
    "    X['protocol_tcp'] = (X['protocol'].str.upper() == 'TCP').astype(int)\n",
    "    X['protocol_udp'] = (X['protocol'].str.upper() == 'UDP').astype(int)\n",
    "    X['protocol_icmp'] = (X['protocol'].str.upper() == 'ICMP').astype(int)\n",
    "    \n",
    "    # Drop original protocol column\n",
    "    X = X.drop(columns=['protocol'])\n",
    "    \n",
    "    print(f\"✓ Created protocol features:\")\n",
    "    print(f\"  - protocol_tcp, protocol_udp, protocol_icmp\")\n",
    "    print(f\"  ✓ Dropped original protocol column\")\n",
    "else:\n",
    "    print(\"⚠️  No protocol column found\")\n",
    "\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# ============================================================\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 4: DROP IDENTIFIER COLUMNS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "identifier_cols = ['flow_id', 'src_ip', 'dst_ip']\n",
    "to_drop = [col for col in identifier_cols if col in X.columns]\n",
    "\n",
    "if to_drop:\n",
    "    print(f\"Dropping identifier columns: {to_drop}\")\n",
    "    X = X.drop(columns=to_drop)\n",
    "else:\n",
    "    print(\"No identifier columns to drop\")\n",
    "\n",
    "# Drop any remaining non-numeric columns\n",
    "non_numeric_cols = X.select_dtypes(exclude=[np.number]).columns\n",
    "if len(non_numeric_cols) > 0:\n",
    "    print(f\"\\n⚠️  Dropping remaining non-numeric columns: {list(non_numeric_cols)}\")\n",
    "    X = X.drop(columns=non_numeric_cols)\n",
    "\n",
    "print(f\"\\n✓ Final feature count: {X.shape[1]}\")\n",
    "print(f\"✓ All features are numeric: {X.select_dtypes(include=[np.number]).shape[1] == X.shape[1]}\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# ============================================================\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Count feature types\n",
    "temporal_features = [col for col in X.columns if any(t in col.lower() for t in ['hour', 'day', 'night', 'weekend', 'business'])]\n",
    "port_features = [col for col in X.columns if 'port' in col.lower()]\n",
    "protocol_features = [col for col in X.columns if 'protocol' in col.lower()]\n",
    "original_features = [col for col in X.columns if col not in temporal_features + port_features + protocol_features]\n",
    "\n",
    "print(f\"Total features: {X.shape[1]}\")\n",
    "print(f\"  - Temporal features: {len(temporal_features)}\")\n",
    "print(f\"  - Port features: {len(port_features)}\")\n",
    "print(f\"  - Protocol features: {len(protocol_features)}\")\n",
    "print(f\"  - Original/derived features: {len(original_features)}\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17612787",
   "metadata": {},
   "source": [
    "## 4. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f502ed06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding target labels...\n",
      "\n",
      "Label mapping:\n",
      "  Benign: 0\n",
      "  Bot: 1\n",
      "\n",
      "Binary distribution (0=Benign, 1=Attack):\n",
      "label\n",
      "0    278864\n",
      "1     10935\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Encode target labels\n",
    "print(\"Encoding target labels...\")\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "print(f\"\\nLabel mapping:\")\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    print(f\"  {label}: {i}\")\n",
    "\n",
    "# Convert to binary if needed (benign vs attack)\n",
    "y_binary = (y != 'Benign').astype(int)\n",
    "print(f\"\\nBinary distribution (0=Benign, 1=Attack):\")\n",
    "print(pd.Series(y_binary).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc10e7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into train/test sets...\n",
      "Training set: (231839, 343)\n",
      "Test set: (57960, 343)\n",
      "\n",
      "Train class distribution:\n",
      "label\n",
      "0    223091\n",
      "1      8748\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test class distribution:\n",
      "label\n",
      "0    55773\n",
      "1     2187\n",
      "Name: count, dtype: int64\n",
      "Training set: (231839, 343)\n",
      "Test set: (57960, 343)\n",
      "\n",
      "Train class distribution:\n",
      "label\n",
      "0    223091\n",
      "1      8748\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test class distribution:\n",
      "label\n",
      "0    55773\n",
      "1     2187\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Split data before scaling to prevent data leakage\n",
    "print(\"Splitting data into train/test sets...\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_binary, test_size=0.2, random_state=42, stratify=y_binary\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"\\nTrain class distribution:\\n{pd.Series(y_train).value_counts()}\")\n",
    "print(f\"\\nTest class distribution:\\n{pd.Series(y_test).value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f87ab8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "IDENTIFYING FEATURES TO SCALE\n",
      "================================================================================\n",
      "Features TO SCALE (continuous): 323\n",
      "Features NOT to scale (binary/categorical): 20\n",
      "\n",
      "Sample continuous features to scale:\n",
      "  - src_port\n",
      "  - dst_port\n",
      "  - duration\n",
      "  - packets_count\n",
      "  - fwd_packets_count\n",
      "  - bwd_packets_count\n",
      "  - total_payload_bytes\n",
      "  - fwd_total_payload_bytes\n",
      "  - bwd_total_payload_bytes\n",
      "  - payload_bytes_max\n",
      "\n",
      "Binary/categorical features (keeping as 0/1):\n",
      "  - is_business_hours\n",
      "  - is_night\n",
      "  - is_weekend\n",
      "  - is_late_night\n",
      "  - dst_port_http\n",
      "  - dst_port_https\n",
      "  - dst_port_ssh\n",
      "  - dst_port_ftp\n",
      "  - dst_port_smtp\n",
      "  - dst_port_dns\n",
      "  - dst_port_telnet\n",
      "  - dst_port_smb\n",
      "  - dst_port_rdp\n",
      "  - dst_port_mysql\n",
      "  - dst_port_postgres\n",
      "  ... and 5 more\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "APPLYING STANDARDSCALER\n",
      "================================================================================\n",
      "Scaling 323 continuous features...\n",
      "Scaling 323 continuous features...\n",
      "✓ Scaling complete\n",
      "✓ 20 binary/categorical features kept as 0/1\n",
      "\n",
      "Scaled training set: (231839, 343)\n",
      "Scaled test set: (57960, 343)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "VERIFICATION: Sample of scaled vs unscaled features\n",
      "================================================================================\n",
      "\n",
      "Continuous feature 'src_port':\n",
      "  Original range: [6.000, 65535.000]\n",
      "  Scaled range: [-2.009, 1.177]\n",
      "  Scaled mean: 0.000 (should be ~0)\n",
      "  Scaled std: 1.000 (should be ~1)\n",
      "\n",
      "Binary feature 'is_business_hours':\n",
      "  Original values: [1]\n",
      "  Scaled values: [1]\n",
      "  ✓ Binary features remain unchanged!\n",
      "================================================================================\n",
      "\n",
      "Sample of scaled features (first 5 rows, first 10 columns):\n",
      "        src_port  dst_port  duration  packets_count  fwd_packets_count  \\\n",
      "244091  1.081177 -0.496640 -0.153053      -0.017458          -0.019853   \n",
      "276234  0.906893 -0.324369 -0.122902      -0.004508          -0.007844   \n",
      "44106   0.412189 -0.517998  0.159728      -0.016019          -0.015850   \n",
      "214029  0.698676 -0.324369 -0.143853      -0.008825          -0.007844   \n",
      "177534  0.461436 -0.496757 -0.127762      -0.007386          -0.007844   \n",
      "\n",
      "        bwd_packets_count  total_payload_bytes  fwd_total_payload_bytes  \\\n",
      "244091          -0.015666            -0.012254                -0.049846   \n",
      "276234          -0.002562            -0.008791                 0.025153   \n",
      "44106           -0.015666            -0.012582                -0.076556   \n",
      "214029          -0.009114            -0.009333                 0.015863   \n",
      "177534          -0.006930            -0.007253                -0.052363   \n",
      "\n",
      "        bwd_total_payload_bytes  payload_bytes_max  \n",
      "244091                -0.011491          -0.642664  \n",
      "276234                -0.009194           0.602281  \n",
      "44106                 -0.011402          -0.532993  \n",
      "214029                -0.009592           0.573357  \n",
      "177534                -0.006445           0.948166  \n",
      "✓ Scaling complete\n",
      "✓ 20 binary/categorical features kept as 0/1\n",
      "\n",
      "Scaled training set: (231839, 343)\n",
      "Scaled test set: (57960, 343)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "VERIFICATION: Sample of scaled vs unscaled features\n",
      "================================================================================\n",
      "\n",
      "Continuous feature 'src_port':\n",
      "  Original range: [6.000, 65535.000]\n",
      "  Scaled range: [-2.009, 1.177]\n",
      "  Scaled mean: 0.000 (should be ~0)\n",
      "  Scaled std: 1.000 (should be ~1)\n",
      "\n",
      "Binary feature 'is_business_hours':\n",
      "  Original values: [1]\n",
      "  Scaled values: [1]\n",
      "  ✓ Binary features remain unchanged!\n",
      "================================================================================\n",
      "\n",
      "Sample of scaled features (first 5 rows, first 10 columns):\n",
      "        src_port  dst_port  duration  packets_count  fwd_packets_count  \\\n",
      "244091  1.081177 -0.496640 -0.153053      -0.017458          -0.019853   \n",
      "276234  0.906893 -0.324369 -0.122902      -0.004508          -0.007844   \n",
      "44106   0.412189 -0.517998  0.159728      -0.016019          -0.015850   \n",
      "214029  0.698676 -0.324369 -0.143853      -0.008825          -0.007844   \n",
      "177534  0.461436 -0.496757 -0.127762      -0.007386          -0.007844   \n",
      "\n",
      "        bwd_packets_count  total_payload_bytes  fwd_total_payload_bytes  \\\n",
      "244091          -0.015666            -0.012254                -0.049846   \n",
      "276234          -0.002562            -0.008791                 0.025153   \n",
      "44106           -0.015666            -0.012582                -0.076556   \n",
      "214029          -0.009114            -0.009333                 0.015863   \n",
      "177534          -0.006930            -0.007253                -0.052363   \n",
      "\n",
      "        bwd_total_payload_bytes  payload_bytes_max  \n",
      "244091                -0.011491          -0.642664  \n",
      "276234                -0.009194           0.602281  \n",
      "44106                 -0.011402          -0.532993  \n",
      "214029                -0.009592           0.573357  \n",
      "177534                -0.006445           0.948166  \n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SMART FEATURE SCALING\n",
    "# ============================================================\n",
    "# Don't scale binary/categorical features (already 0/1)\n",
    "# Only scale continuous features\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"IDENTIFYING FEATURES TO SCALE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Identify binary features that should NOT be scaled\n",
    "binary_feature_patterns = [\n",
    "    'is_business_hours', 'is_night', 'is_weekend', 'is_late_night',\n",
    "    'dst_port_http', 'dst_port_https', 'dst_port_ssh', 'dst_port_ftp',\n",
    "    'dst_port_smtp', 'dst_port_dns', 'dst_port_telnet', 'dst_port_smb',\n",
    "    'dst_port_rdp', 'dst_port_mysql', 'dst_port_postgres',\n",
    "    'protocol_tcp', 'protocol_udp', 'protocol_icmp',\n",
    "    'src_port_is_privileged', 'src_port_is_ephemeral',\n",
    "]\n",
    "\n",
    "# Also don't scale one-hot encoded features\n",
    "one_hot_patterns = ['dst_port_cat_', 'protocol_']\n",
    "\n",
    "# Find features that should NOT be scaled\n",
    "no_scale_features = []\n",
    "for col in X_train.columns:\n",
    "    # Check if it matches any binary pattern\n",
    "    if col in binary_feature_patterns:\n",
    "        no_scale_features.append(col)\n",
    "    # Check if it's a one-hot encoded feature\n",
    "    elif any(pattern in col for pattern in one_hot_patterns):\n",
    "        no_scale_features.append(col)\n",
    "\n",
    "# Features TO scale (continuous variables)\n",
    "scale_features = [col for col in X_train.columns if col not in no_scale_features]\n",
    "\n",
    "print(f\"Features TO SCALE (continuous): {len(scale_features)}\")\n",
    "print(f\"Features NOT to scale (binary/categorical): {len(no_scale_features)}\")\n",
    "\n",
    "if len(scale_features) > 0:\n",
    "    print(f\"\\nSample continuous features to scale:\")\n",
    "    for feat in scale_features[:10]:\n",
    "        print(f\"  - {feat}\")\n",
    "\n",
    "if len(no_scale_features) > 0:\n",
    "    print(f\"\\nBinary/categorical features (keeping as 0/1):\")\n",
    "    for feat in no_scale_features[:15]:\n",
    "        print(f\"  - {feat}\")\n",
    "    if len(no_scale_features) > 15:\n",
    "        print(f\"  ... and {len(no_scale_features) - 15} more\")\n",
    "\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# ============================================================\n",
    "print(\"=\"*80)\n",
    "print(\"APPLYING STANDARDSCALER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Create copies to avoid modifying original\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "# Scale ONLY the continuous features\n",
    "if len(scale_features) > 0:\n",
    "    print(f\"Scaling {len(scale_features)} continuous features...\")\n",
    "    X_train_scaled[scale_features] = scaler.fit_transform(X_train[scale_features])\n",
    "    X_test_scaled[scale_features] = scaler.transform(X_test[scale_features])\n",
    "    print(\"✓ Scaling complete\")\n",
    "else:\n",
    "    print(\"⚠️  No continuous features to scale\")\n",
    "\n",
    "# Binary features remain unchanged (already 0/1)\n",
    "if len(no_scale_features) > 0:\n",
    "    print(f\"✓ {len(no_scale_features)} binary/categorical features kept as 0/1\")\n",
    "\n",
    "print(f\"\\nScaled training set: {X_train_scaled.shape}\")\n",
    "print(f\"Scaled test set: {X_test_scaled.shape}\")\n",
    "\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# ============================================================\n",
    "print(\"=\"*80)\n",
    "print(\"VERIFICATION: Sample of scaled vs unscaled features\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show example of a scaled continuous feature\n",
    "if len(scale_features) > 0:\n",
    "    example_continuous = scale_features[0]\n",
    "    print(f\"\\nContinuous feature '{example_continuous}':\")\n",
    "    print(f\"  Original range: [{X_train[example_continuous].min():.3f}, {X_train[example_continuous].max():.3f}]\")\n",
    "    print(f\"  Scaled range: [{X_train_scaled[example_continuous].min():.3f}, {X_train_scaled[example_continuous].max():.3f}]\")\n",
    "    print(f\"  Scaled mean: {X_train_scaled[example_continuous].mean():.3f} (should be ~0)\")\n",
    "    print(f\"  Scaled std: {X_train_scaled[example_continuous].std():.3f} (should be ~1)\")\n",
    "\n",
    "# Show example of a binary feature\n",
    "if len(no_scale_features) > 0:\n",
    "    example_binary = no_scale_features[0]\n",
    "    print(f\"\\nBinary feature '{example_binary}':\")\n",
    "    print(f\"  Original values: {X_train[example_binary].unique()}\")\n",
    "    print(f\"  Scaled values: {X_train_scaled[example_binary].unique()}\")\n",
    "    print(f\"  ✓ Binary features remain unchanged!\")\n",
    "\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Show sample of final scaled data\n",
    "print(\"Sample of scaled features (first 5 rows, first 10 columns):\")\n",
    "print(X_train_scaled.iloc[:5, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea4e6873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "VALIDATING ENGINEERED FEATURES\n",
      "================================================================================\n",
      "\n",
      "✓ NaN values: 0\n",
      "✓ Infinite values: 0\n",
      "✓ All features are valid!\n",
      "\n",
      "================================================================================\n",
      "SAMPLE FEATURE DISTRIBUTIONS\n",
      "================================================================================\n",
      "\n",
      "Temporal feature distributions:\n",
      "\n",
      "hour:\n",
      "hour\n",
      "0.0    231839\n",
      "Name: count, dtype: int64\n",
      "\n",
      "day_of_week:\n",
      "day_of_week\n",
      "0.0    231839\n",
      "Name: count, dtype: int64\n",
      "\n",
      "day_of_month:\n",
      "day_of_month\n",
      "0.0    231839\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Port feature distributions:\n",
      "\n",
      "dst_port_http:\n",
      "dst_port_http\n",
      "0    198839\n",
      "1     33000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "dst_port_https:\n",
      "dst_port_https\n",
      "0    180625\n",
      "1     51214\n",
      "Name: count, dtype: int64\n",
      "\n",
      "dst_port_ssh:\n",
      "dst_port_ssh\n",
      "0    230976\n",
      "1       863\n",
      "Name: count, dtype: int64\n",
      "\n",
      "================================================================================\n",
      "✓ FEATURE ENGINEERING COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "Final dataset ready for training:\n",
      "  Training samples: 231,839\n",
      "  Test samples: 57,960\n",
      "  Total features: 343\n",
      "  Class distribution (train): {0: 223091, 1: 8748}\n",
      "================================================================================\n",
      "\n",
      "✓ NaN values: 0\n",
      "✓ Infinite values: 0\n",
      "✓ All features are valid!\n",
      "\n",
      "================================================================================\n",
      "SAMPLE FEATURE DISTRIBUTIONS\n",
      "================================================================================\n",
      "\n",
      "Temporal feature distributions:\n",
      "\n",
      "hour:\n",
      "hour\n",
      "0.0    231839\n",
      "Name: count, dtype: int64\n",
      "\n",
      "day_of_week:\n",
      "day_of_week\n",
      "0.0    231839\n",
      "Name: count, dtype: int64\n",
      "\n",
      "day_of_month:\n",
      "day_of_month\n",
      "0.0    231839\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Port feature distributions:\n",
      "\n",
      "dst_port_http:\n",
      "dst_port_http\n",
      "0    198839\n",
      "1     33000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "dst_port_https:\n",
      "dst_port_https\n",
      "0    180625\n",
      "1     51214\n",
      "Name: count, dtype: int64\n",
      "\n",
      "dst_port_ssh:\n",
      "dst_port_ssh\n",
      "0    230976\n",
      "1       863\n",
      "Name: count, dtype: int64\n",
      "\n",
      "================================================================================\n",
      "✓ FEATURE ENGINEERING COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "Final dataset ready for training:\n",
      "  Training samples: 231,839\n",
      "  Test samples: 57,960\n",
      "  Total features: 343\n",
      "  Class distribution (train): {0: 223091, 1: 8748}\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FEATURE VALIDATION\n",
    "# ============================================================\n",
    "# Quick sanity checks on the engineered features\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"VALIDATING ENGINEERED FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check for any NaN or infinite values\n",
    "nan_count = X_train_scaled.isna().sum().sum()\n",
    "inf_count = np.isinf(X_train_scaled.select_dtypes(include=[np.number])).sum().sum()\n",
    "\n",
    "print(f\"\\n✓ NaN values: {nan_count}\")\n",
    "print(f\"✓ Infinite values: {inf_count}\")\n",
    "\n",
    "if nan_count > 0 or inf_count > 0:\n",
    "    print(\"⚠️  WARNING: Found NaN or infinite values!\")\n",
    "else:\n",
    "    print(\"✓ All features are valid!\")\n",
    "\n",
    "# Show distribution of some key features\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE FEATURE DISTRIBUTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Temporal features\n",
    "temporal_features = [col for col in X_train_scaled.columns if any(t in col.lower() for t in ['hour', 'day', 'night', 'weekend'])]\n",
    "if temporal_features:\n",
    "    print(f\"\\nTemporal feature distributions:\")\n",
    "    sample_temporal = temporal_features[:3]\n",
    "    for feat in sample_temporal:\n",
    "        print(f\"\\n{feat}:\")\n",
    "        print(X_train_scaled[feat].value_counts().head())\n",
    "\n",
    "# Port features\n",
    "port_features = [col for col in X_train_scaled.columns if 'dst_port' in col.lower() and any(s in col for s in ['http', 'ssh', 'https'])]\n",
    "if port_features:\n",
    "    print(f\"\\nPort feature distributions:\")\n",
    "    for feat in port_features[:3]:\n",
    "        print(f\"\\n{feat}:\")\n",
    "        print(X_train_scaled[feat].value_counts())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ FEATURE ENGINEERING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nFinal dataset ready for training:\")\n",
    "print(f\"  Training samples: {X_train_scaled.shape[0]:,}\")\n",
    "print(f\"  Test samples: {X_test_scaled.shape[0]:,}\")\n",
    "print(f\"  Total features: {X_train_scaled.shape[1]}\")\n",
    "print(f\"  Class distribution (train): {pd.Series(y_train).value_counts().to_dict()}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a380f3",
   "metadata": {},
   "source": [
    "## 5. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba6fa309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving processed data...\n",
      "\n",
      "Processed data saved to: /Users/matthewweaver/Repositories/nidstream/data/processed\n",
      "Files created:\n",
      "  - X_train.csv\n",
      "  - X_test.csv\n",
      "  - y_train.csv\n",
      "  - y_test.csv\n",
      "  - scaler.pkl\n",
      "  - label_encoder.pkl\n",
      "\n",
      "Processed data saved to: /Users/matthewweaver/Repositories/nidstream/data/processed\n",
      "Files created:\n",
      "  - X_train.csv\n",
      "  - X_test.csv\n",
      "  - y_train.csv\n",
      "  - y_test.csv\n",
      "  - scaler.pkl\n",
      "  - label_encoder.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save processed data\n",
    "print(\"Saving processed data...\")\n",
    "\n",
    "processed_dir = project_root / 'data' / 'processed'\n",
    "processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save train/test splits\n",
    "X_train_scaled.to_csv(processed_dir / 'X_train.csv', index=False)\n",
    "X_test_scaled.to_csv(processed_dir / 'X_test.csv', index=False)\n",
    "pd.Series(y_train, name='label').to_csv(processed_dir / 'y_train.csv', index=False)\n",
    "pd.Series(y_test, name='label').to_csv(processed_dir / 'y_test.csv', index=False)\n",
    "\n",
    "# Save scaler and label encoder for later use\n",
    "import joblib\n",
    "joblib.dump(scaler, processed_dir / 'scaler.pkl')\n",
    "joblib.dump(label_encoder, processed_dir / 'label_encoder.pkl')\n",
    "\n",
    "print(f\"\\nProcessed data saved to: {processed_dir}\")\n",
    "print(\"Files created:\")\n",
    "print(\"  - X_train.csv\")\n",
    "print(\"  - X_test.csv\")\n",
    "print(\"  - y_train.csv\")\n",
    "print(\"  - y_test.csv\")\n",
    "print(\"  - scaler.pkl\")\n",
    "print(\"  - label_encoder.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12999b7",
   "metadata": {},
   "source": [
    "## 6. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73612f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FEATURE ENGINEERING SUMMARY\n",
      "============================================================\n",
      "\n",
      "Original dataset: 289,799 samples, 323 features\n",
      "Final feature count: 343\n",
      "\n",
      "Training set: 231,839 samples\n",
      "Test set: 57,960 samples\n",
      "\n",
      "Class distribution (train):\n",
      "  Benign: 223,091 (96.2%)\n",
      "  Attack: 8,748 (3.8%)\n",
      "\n",
      "Data ready for model training!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Summary of feature engineering process\n",
    "print(\"=\" * 60)\n",
    "print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nOriginal dataset: {len(df):,} samples, {len(df.columns)} features\")\n",
    "print(f\"Final feature count: {X_train_scaled.shape[1]}\")\n",
    "print(f\"\\nTraining set: {len(X_train_scaled):,} samples\")\n",
    "print(f\"Test set: {len(X_test_scaled):,} samples\")\n",
    "print(f\"\\nClass distribution (train):\")\n",
    "print(f\"  Benign: {(y_train == 0).sum():,} ({(y_train == 0).sum() / len(y_train) * 100:.1f}%)\")\n",
    "print(f\"  Attack: {(y_train == 1).sum():,} ({(y_train == 1).sum() / len(y_train) * 100:.1f}%)\")\n",
    "print(f\"\\nData ready for model training!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c231c7",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "The processed data is now ready for:\n",
    "1. Model training in `03_model_training.ipynb`\n",
    "2. Hyperparameter tuning\n",
    "3. Model evaluation and comparison\n",
    "\n",
    "**Note:** You may want to:\n",
    "- Apply SMOTE or other techniques for class imbalance\n",
    "- Perform feature selection to reduce dimensionality\n",
    "- Experiment with different scaling methods\n",
    "- Create more domain-specific features based on network traffic analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nidstream",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
