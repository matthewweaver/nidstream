{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1b611aa",
   "metadata": {},
   "source": [
    "# Feature Engineering for Network Intrusion Detection\n",
    "\n",
    "This notebook performs feature engineering on the BCCC-CSE-CIC-IDS2018 dataset.\n",
    "\n",
    "## Objectives:\n",
    "1. Load and preprocess raw network flow data\n",
    "2. Handle missing values and outliers\n",
    "3. Create derived features\n",
    "4. Encode categorical variables\n",
    "5. Scale numerical features\n",
    "6. Handle class imbalance\n",
    "7. Save processed features for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e7f6d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22f5cc4",
   "metadata": {},
   "source": [
    "## 1. Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4124e2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w3/2s_c8kqx3636yg6zdrl9gcnm0000gn/T/ipykernel_86793/2173347490.py:5: DtypeWarning: Columns (213,214) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_path)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w3/2s_c8kqx3636yg6zdrl9gcnm0000gn/T/ipykernel_86793/2173347490.py:5: DtypeWarning: Columns (213,214) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_path)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m project_root = Path().resolve()\n\u001b[32m      3\u001b[39m data_path = project_root / \u001b[33m'\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m'\u001b[39m / \u001b[33m'\u001b[39m\u001b[33mraw\u001b[39m\u001b[33m'\u001b[39m / \u001b[33m'\u001b[39m\u001b[33mfriday_02_03_2018_combined.csv\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m records\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNumber of features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df.columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repositories/nidstream/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repositories/nidstream/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repositories/nidstream/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1968\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1965\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1966\u001b[39m         new_col_dict = col_dict\n\u001b[32m-> \u001b[39m\u001b[32m1968\u001b[39m     df = \u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnew_col_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1971\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1973\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1975\u001b[39m     \u001b[38;5;28mself\u001b[39m._currow += new_rows\n\u001b[32m   1976\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repositories/nidstream/.venv/lib/python3.11/site-packages/pandas/core/frame.py:782\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    776\u001b[39m     mgr = \u001b[38;5;28mself\u001b[39m._init_mgr(\n\u001b[32m    777\u001b[39m         data, axes={\u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m: index, \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: columns}, dtype=dtype, copy=copy\n\u001b[32m    778\u001b[39m     )\n\u001b[32m    780\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    781\u001b[39m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m782\u001b[39m     mgr = \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    783\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma.MaskedArray):\n\u001b[32m    784\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repositories/nidstream/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py:503\u001b[39m, in \u001b[36mdict_to_mgr\u001b[39m\u001b[34m(data, index, columns, dtype, typ, copy)\u001b[39m\n\u001b[32m    499\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    500\u001b[39m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[32m    501\u001b[39m         arrays = [x.copy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repositories/nidstream/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py:152\u001b[39m, in \u001b[36marrays_to_mgr\u001b[39m\u001b[34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[39m\n\u001b[32m    149\u001b[39m axes = [columns, index]\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ == \u001b[33m\"\u001b[39m\u001b[33mblock\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_block_manager_from_column_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m        \u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrefs\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m typ == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ArrayManager(arrays, [index, columns])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repositories/nidstream/.venv/lib/python3.11/site-packages/pandas/core/internals/managers.py:2158\u001b[39m, in \u001b[36mcreate_block_manager_from_column_arrays\u001b[39m\u001b[34m(arrays, axes, consolidate, refs)\u001b[39m\n\u001b[32m   2140\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_block_manager_from_column_arrays\u001b[39m(\n\u001b[32m   2141\u001b[39m     arrays: \u001b[38;5;28mlist\u001b[39m[ArrayLike],\n\u001b[32m   2142\u001b[39m     axes: \u001b[38;5;28mlist\u001b[39m[Index],\n\u001b[32m   (...)\u001b[39m\u001b[32m   2154\u001b[39m     \u001b[38;5;66;03m# These last three are sufficient to allow us to safely pass\u001b[39;00m\n\u001b[32m   2155\u001b[39m     \u001b[38;5;66;03m#  verify_integrity=False below.\u001b[39;00m\n\u001b[32m   2157\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2158\u001b[39m         blocks = \u001b[43m_form_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2159\u001b[39m         mgr = BlockManager(blocks, axes, verify_integrity=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   2160\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repositories/nidstream/.venv/lib/python3.11/site-packages/pandas/core/internals/managers.py:2231\u001b[39m, in \u001b[36m_form_blocks\u001b[39m\u001b[34m(arrays, consolidate, refs)\u001b[39m\n\u001b[32m   2228\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(dtype.type, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m)):\n\u001b[32m   2229\u001b[39m     dtype = np.dtype(\u001b[38;5;28mobject\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2231\u001b[39m values, placement = \u001b[43m_stack_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtup_block\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_dtlike:\n\u001b[32m   2233\u001b[39m     values = ensure_wrapped_if_datetimelike(values)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repositories/nidstream/.venv/lib/python3.11/site-packages/pandas/core/internals/managers.py:2273\u001b[39m, in \u001b[36m_stack_arrays\u001b[39m\u001b[34m(tuples, dtype)\u001b[39m\n\u001b[32m   2271\u001b[39m stacked = np.empty(shape, dtype=dtype)\n\u001b[32m   2272\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, arr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(arrays):\n\u001b[32m-> \u001b[39m\u001b[32m2273\u001b[39m     stacked[i] = arr\n\u001b[32m   2275\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m stacked, placement\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "project_root = Path().resolve()\n",
    "data_path = project_root / 'data' / 'raw' / 'friday_02_03_2018_combined_sample.csv'\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"Loaded {len(df):,} records\")\n",
    "print(f\"Number of features: {len(df.columns)}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aae62fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 289799 entries, 0 to 289798\n",
      "Columns: 323 entries, flow_id to label\n",
      "dtypes: float64(259), int64(56), object(8)\n",
      "memory usage: 714.2+ MB\n",
      "None\n",
      "\n",
      "Missing values:\n",
      "payload_bytes_cov                  108358\n",
      "fwd_payload_bytes_cov               75401\n",
      "bwd_payload_bytes_cov               59023\n",
      "bwd_packets_IAT_skewness            49868\n",
      "bwd_packets_IAT_cov                 49868\n",
      "fwd_packets_IAT_cov                 34082\n",
      "fwd_packets_IAT_skewness            34027\n",
      "cov_payload_bytes_delta_len         28349\n",
      "cov_fwd_payload_bytes_delta_len     18905\n",
      "cov_bwd_payload_bytes_delta_len     15945\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check data types and missing values\n",
    "print(\"Data Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2891990f",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc02c177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'label' as target variable\n",
      "\n",
      "Feature matrix shape: (289799, 322)\n",
      "Target shape: (289799,)\n",
      "\n",
      "Class distribution:\n",
      "label\n",
      "Benign    278864\n",
      "Bot        10935\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "# Identify the label column (could be 'label', 'Label', etc.)\n",
    "label_col = \"label\"\n",
    "\n",
    "print(f\"Using '{label_col}' as target variable\")\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop(columns=[label_col])\n",
    "y = df[label_col]\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nClass distribution:\\n{y.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5dbfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling missing values...\n",
      "\n",
      "Columns with missing values:\n",
      "payload_bytes_cov                  108358\n",
      "fwd_payload_bytes_cov               75401\n",
      "bwd_payload_bytes_cov               59023\n",
      "packets_IAT_cov                       437\n",
      "fwd_packets_IAT_skewness            34027\n",
      "fwd_packets_IAT_cov                 34082\n",
      "bwd_packets_IAT_skewness            49868\n",
      "bwd_packets_IAT_cov                 49868\n",
      "cov_packets_delta_time                437\n",
      "cov_bwd_packets_delta_time              1\n",
      "cov_fwd_packets_delta_time             55\n",
      "cov_packets_delta_len                5696\n",
      "cov_bwd_packets_delta_len            1398\n",
      "cov_fwd_packets_delta_len            3102\n",
      "cov_header_bytes_delta_len           7780\n",
      "cov_bwd_header_bytes_delta_len       1464\n",
      "cov_fwd_header_bytes_delta_len       4140\n",
      "cov_payload_bytes_delta_len         28349\n",
      "cov_bwd_payload_bytes_delta_len     15945\n",
      "cov_fwd_payload_bytes_delta_len     18905\n",
      "dtype: int64\n",
      "\n",
      "Remaining missing values: 0\n",
      "\n",
      "Final feature matrix shape: (289799, 322)\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values\n",
    "print(\"Handling missing values...\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_counts = X.isnull().sum()\n",
    "missing_cols = missing_counts[missing_counts > 0]\n",
    "\n",
    "if len(missing_cols) > 0:\n",
    "    print(f\"\\nColumns with missing values:\\n{missing_cols}\")\n",
    "    \n",
    "    # Strategy: Fill numeric columns with median, drop columns with >50% missing\n",
    "    threshold = 0.5\n",
    "    high_missing = missing_cols[missing_cols / len(X) > threshold]\n",
    "    \n",
    "    if len(high_missing) > 0:\n",
    "        print(f\"\\nDropping columns with >{threshold*100}% missing: {list(high_missing.index)}\")\n",
    "        X = X.drop(columns=high_missing.index)\n",
    "    \n",
    "    # Fill remaining missing values with median for numeric columns\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if X[col].isnull().any():\n",
    "            X[col] = X[col].fillna(X[col].median())\n",
    "    \n",
    "    print(f\"\\nRemaining missing values: {X.isnull().sum().sum()}\")\n",
    "else:\n",
    "    print(\"No missing values found!\")\n",
    "\n",
    "print(f\"\\nFinal feature matrix shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2375d115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for infinite values...\n",
      "\n",
      "Replaced infinite values in 9 columns\n",
      "  cov_packets_delta_len: 998 infinite values\n",
      "  cov_bwd_packets_delta_len: 1093 infinite values\n",
      "  cov_fwd_packets_delta_len: 141 infinite values\n",
      "  cov_header_bytes_delta_len: 488 infinite values\n",
      "  cov_bwd_header_bytes_delta_len: 1102 infinite values\n",
      "  cov_fwd_header_bytes_delta_len: 63 infinite values\n",
      "  cov_payload_bytes_delta_len: 204435 infinite values\n",
      "  cov_bwd_payload_bytes_delta_len: 94260 infinite values\n",
      "  cov_fwd_payload_bytes_delta_len: 183487 infinite values\n"
     ]
    }
   ],
   "source": [
    "# Handle infinite values\n",
    "print(\"Checking for infinite values...\")\n",
    "\n",
    "numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "inf_counts = {}\n",
    "\n",
    "for col in numeric_cols:\n",
    "    inf_count = np.isinf(X[col]).sum()\n",
    "    if inf_count > 0:\n",
    "        inf_counts[col] = inf_count\n",
    "        # Replace inf with NaN, then fill with column median\n",
    "        X[col] = X[col].replace([np.inf, -np.inf], np.nan)\n",
    "        X[col] = X[col].fillna(X[col].median())\n",
    "\n",
    "if inf_counts:\n",
    "    print(f\"\\nReplaced infinite values in {len(inf_counts)} columns\")\n",
    "    for col, count in list(inf_counts.items())[:10]:\n",
    "        print(f\"  {col}: {count} infinite values\")\n",
    "else:\n",
    "    print(\"No infinite values found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ade653e",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc08614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 1: DROP TIMESTAMP COLUMN\n",
      "================================================================================\n",
      "  ‚úì Dropped timestamp column\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "STEP 2: PORT FEATURE ENGINEERING\n",
      "================================================================================\n",
      "Encoding destination port (target service)...\n",
      "\n",
      "Creating port range one-hot features...\n",
      "  dst_port_cat_well_known: 132938 flows\n",
      "  dst_port_cat_registered: 115616 flows\n",
      "  dst_port_cat_ephemeral: 41245 flows\n",
      "\n",
      "‚úì Created destination port features:\n",
      "  - Binary flags for common services: http, https, ssh, ftp, smtp, dns, etc.\n",
      "  - Port range one-hot features: well_known, registered, ephemeral\n",
      "  ‚úì Dropped original dst_port column\n",
      "\n",
      "‚úì All dst_port columns created (14):\n",
      "  - dst_port_cat_ephemeral\n",
      "  - dst_port_cat_registered\n",
      "  - dst_port_cat_well_known\n",
      "  - dst_port_dns\n",
      "  - dst_port_ftp\n",
      "  - dst_port_http\n",
      "  - dst_port_https\n",
      "  - dst_port_mysql\n",
      "  - dst_port_postgres\n",
      "  - dst_port_rdp\n",
      "  - dst_port_smb\n",
      "  - dst_port_smtp\n",
      "  - dst_port_ssh\n",
      "  - dst_port_telnet\n",
      "\n",
      "Handling source port...\n",
      "‚úì Created source port features:\n",
      "  - src_port_is_privileged (<1024)\n",
      "  - src_port_is_ephemeral (>=49152)\n",
      "  ‚úì Dropped original src_port column\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "STEP 3: PROTOCOL ENCODING\n",
      "================================================================================\n",
      "Protocol values: ['TCP']\n",
      "‚úì Created protocol features:\n",
      "  - protocol_tcp, protocol_udp, protocol_icmp\n",
      "  ‚úì Dropped original protocol column\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "STEP 4: DROP IDENTIFIER COLUMNS\n",
      "================================================================================\n",
      "Dropping identifier columns: ['flow_id', 'src_ip', 'dst_ip']\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "STEP 5: HANDLE MIXED-TYPE NUMERIC COLUMNS\n",
      "================================================================================\n",
      "Converting all mixed-type columns to numeric...\n",
      "\n",
      "Found 2 non-numeric columns to convert:\n",
      "\n",
      "Processing 'delta_start'...\n",
      "  Found 97970 non-numeric values\n",
      "  Sample values: ['not a complete handshake']\n",
      "  ‚úì Converted to numeric, filled 97970 values with median: 0.000200\n",
      "\n",
      "Processing 'handshake_duration'...\n",
      "  Found 97970 non-numeric values\n",
      "  Sample values: ['not a complete handshake']\n",
      "  ‚úì Converted to numeric, filled 97970 values with median: 0.105300\n",
      "\n",
      "‚úì Final feature count: 334\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "FEATURE ENGINEERING SUMMARY\n",
      "================================================================================\n",
      "Total features: 334\n",
      "  - Port features: 16\n",
      "  - Protocol features: 3\n",
      "  - Original/derived features: 315\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "COLUMN CHANGES SUMMARY\n",
      "================================================================================\n",
      "\n",
      "üìä COLUMNS REMOVED (7):\n",
      "  ‚ùå dst_ip\n",
      "  ‚ùå dst_port\n",
      "  ‚ùå flow_id\n",
      "  ‚ùå protocol\n",
      "  ‚ùå src_ip\n",
      "  ‚ùå src_port\n",
      "  ‚ùå timestamp\n",
      "\n",
      "üìä COLUMNS ADDED (19):\n",
      "  ‚úÖ dst_port_cat_ephemeral\n",
      "  ‚úÖ dst_port_cat_registered\n",
      "  ‚úÖ dst_port_cat_well_known\n",
      "  ‚úÖ dst_port_dns\n",
      "  ‚úÖ dst_port_ftp\n",
      "  ‚úÖ dst_port_http\n",
      "  ‚úÖ dst_port_https\n",
      "  ‚úÖ dst_port_mysql\n",
      "  ‚úÖ dst_port_postgres\n",
      "  ‚úÖ dst_port_rdp\n",
      "  ‚úÖ dst_port_smb\n",
      "  ‚úÖ dst_port_smtp\n",
      "  ‚úÖ dst_port_ssh\n",
      "  ‚úÖ dst_port_telnet\n",
      "  ‚úÖ protocol_icmp\n",
      "  ‚úÖ protocol_tcp\n",
      "  ‚úÖ protocol_udp\n",
      "  ‚úÖ src_port_is_ephemeral\n",
      "  ‚úÖ src_port_is_privileged\n",
      "\n",
      "üìä NET CHANGE: 334 - 322 = +12 columns\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STEP 1: DROP TIMESTAMP COLUMN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Store original columns for comparison at the end\n",
    "original_columns = set(X.columns)\n",
    "\n",
    "# Drop timestamp - not using temporal features for now\n",
    "X = X.drop(columns=['timestamp'])\n",
    "print(f\"  ‚úì Dropped timestamp column\")\n",
    "\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# ============================================================\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 2: PORT FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"Encoding destination port (target service)...\")\n",
    "    \n",
    "# Well-known ports (these are the ones attackers often target)\n",
    "X['dst_port_http'] = (X['dst_port'].isin([80, 8080, 8000, 8888])).astype(int)\n",
    "X['dst_port_https'] = (X['dst_port'] == 443).astype(int)\n",
    "X['dst_port_ssh'] = (X['dst_port'] == 22).astype(int)\n",
    "X['dst_port_ftp'] = (X['dst_port'].isin([20, 21])).astype(int)\n",
    "X['dst_port_smtp'] = (X['dst_port'].isin([25, 587, 465])).astype(int)\n",
    "X['dst_port_dns'] = (X['dst_port'] == 53).astype(int)\n",
    "X['dst_port_telnet'] = (X['dst_port'] == 23).astype(int)\n",
    "X['dst_port_smb'] = (X['dst_port'].isin([139, 445])).astype(int)\n",
    "X['dst_port_rdp'] = (X['dst_port'] == 3389).astype(int)\n",
    "X['dst_port_mysql'] = (X['dst_port'] == 3306).astype(int)\n",
    "X['dst_port_postgres'] = (X['dst_port'] == 5432).astype(int)\n",
    "\n",
    "# Port range categories - create as separate binary features (one-hot encoding)\n",
    "print(\"\\nCreating port range one-hot features...\")\n",
    "X['dst_port_cat_well_known'] = (X['dst_port'] < 1024).astype(int)\n",
    "X['dst_port_cat_registered'] = ((X['dst_port'] >= 1024) & (X['dst_port'] < 49152)).astype(int)\n",
    "X['dst_port_cat_ephemeral'] = (X['dst_port'] >= 49152).astype(int)\n",
    "\n",
    "# Print the counts for each category\n",
    "print(f\"  dst_port_cat_well_known: {X['dst_port_cat_well_known'].sum()} flows\")\n",
    "print(f\"  dst_port_cat_registered: {X['dst_port_cat_registered'].sum()} flows\")\n",
    "print(f\"  dst_port_cat_ephemeral: {X['dst_port_cat_ephemeral'].sum()} flows\")\n",
    "\n",
    "# Drop original dst_port\n",
    "X = X.drop(columns=['dst_port'])\n",
    "\n",
    "print(f\"\\n‚úì Created destination port features:\")\n",
    "print(f\"  - Binary flags for common services: http, https, ssh, ftp, smtp, dns, etc.\")\n",
    "print(f\"  - Port range one-hot features: well_known, registered, ephemeral\")\n",
    "print(f\"  ‚úì Dropped original dst_port column\")\n",
    "\n",
    "# List all dst_port columns created\n",
    "dst_port_cols = [col for col in X.columns if col.startswith('dst_port')]\n",
    "print(f\"\\n‚úì All dst_port columns created ({len(dst_port_cols)}):\")\n",
    "for col in sorted(dst_port_cols):\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "print(\"\\nHandling source port...\")\n",
    "    \n",
    "# Source port is usually ephemeral (random), less predictive\n",
    "# But we can create some useful features\n",
    "X['src_port_is_privileged'] = (X['src_port'] < 1024).astype(int)\n",
    "X['src_port_is_ephemeral'] = (X['src_port'] >= 49152).astype(int)\n",
    "X = X.drop(columns=['src_port'])\n",
    "\n",
    "print(f\"‚úì Created source port features:\")\n",
    "print(f\"  - src_port_is_privileged (<1024)\")\n",
    "print(f\"  - src_port_is_ephemeral (>=49152)\")\n",
    "print(f\"  ‚úì Dropped original src_port column\")\n",
    "\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# ============================================================\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 3: PROTOCOL ENCODING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"Protocol values: {X['protocol'].unique()}\")\n",
    "    \n",
    "# Common protocols\n",
    "X['protocol_tcp'] = (X['protocol'].str.upper() == 'TCP').astype(int)\n",
    "X['protocol_udp'] = (X['protocol'].str.upper() == 'UDP').astype(int)\n",
    "X['protocol_icmp'] = (X['protocol'].str.upper() == 'ICMP').astype(int)\n",
    "\n",
    "# Drop original protocol column\n",
    "X = X.drop(columns=['protocol'])\n",
    "\n",
    "print(f\"‚úì Created protocol features:\")\n",
    "print(f\"  - protocol_tcp, protocol_udp, protocol_icmp\")\n",
    "print(f\"  ‚úì Dropped original protocol column\")\n",
    "\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 4: DROP IDENTIFIER COLUMNS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "identifier_cols = ['flow_id', 'src_ip', 'dst_ip']\n",
    "to_drop = [col for col in identifier_cols if col in X.columns]\n",
    "\n",
    "if to_drop:\n",
    "    print(f\"Dropping identifier columns: {to_drop}\")\n",
    "    X = X.drop(columns=to_drop)\n",
    "else:\n",
    "    print(\"No identifier columns to drop\")\n",
    "\n",
    "\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 5: HANDLE MIXED-TYPE NUMERIC COLUMNS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Handle all columns that should be numeric but contain text values\n",
    "# This includes delta_start and any other similar columns\n",
    "print(\"Converting all mixed-type columns to numeric...\")\n",
    "\n",
    "# Get all non-numeric columns (excluding ones we just created which are binary)\n",
    "non_numeric_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "if len(non_numeric_cols) > 0:\n",
    "    print(f\"\\nFound {len(non_numeric_cols)} non-numeric columns to convert:\")\n",
    "    \n",
    "    for col in non_numeric_cols:\n",
    "        print(f\"\\nProcessing '{col}'...\")\n",
    "        \n",
    "        # Count non-numeric values before conversion\n",
    "        non_numeric_mask = pd.to_numeric(X[col], errors='coerce').isna() & X[col].notna()\n",
    "        non_numeric_count = non_numeric_mask.sum()\n",
    "        \n",
    "        if non_numeric_count > 0:\n",
    "            print(f\"  Found {non_numeric_count} non-numeric values\")\n",
    "            sample_values = X.loc[non_numeric_mask, col].unique()[:3]\n",
    "            print(f\"  Sample values: {sample_values}\")\n",
    "        \n",
    "        # Convert to numeric, coercing errors to NaN\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "        \n",
    "        # Fill NaN values with median of valid numeric values\n",
    "        if X[col].notna().any():\n",
    "            median_val = X[col].median()\n",
    "            X[col] = X[col].fillna(median_val)\n",
    "            if non_numeric_count > 0:\n",
    "                print(f\"  ‚úì Converted to numeric, filled {non_numeric_count} values with median: {median_val:.6f}\")\n",
    "            else:\n",
    "                print(f\"  ‚úì Already numeric, filled NaN with median: {median_val:.6f}\")\n",
    "        else:\n",
    "            # If all values were non-numeric, fill with 0\n",
    "            X[col] = X[col].fillna(0)\n",
    "            print(f\"  ‚úì Converted to numeric, filled all NaN values with 0\")\n",
    "else:\n",
    "    print(\"All columns are already numeric!\")\n",
    "print(f\"\\n‚úì Final feature count: {X.shape[1]}\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Count feature types\n",
    "port_features = [col for col in X.columns if 'port' in col.lower()]\n",
    "protocol_features = [col for col in X.columns if 'protocol' in col.lower()]\n",
    "original_features = [col for col in X.columns if col not in port_features + protocol_features]\n",
    "\n",
    "print(f\"Total features: {X.shape[1]}\")\n",
    "print(f\"  - Port features: {len(port_features)}\")\n",
    "print(f\"  - Protocol features: {len(protocol_features)}\")\n",
    "print(f\"  - Original/derived features: {len(original_features)}\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# ============================================================\n",
    "print(\"=\"*80)\n",
    "print(\"COLUMN CHANGES SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate which columns were added and removed\n",
    "final_columns = set(X.columns)\n",
    "added_columns = sorted(final_columns - original_columns)\n",
    "removed_columns = sorted(original_columns - final_columns)\n",
    "\n",
    "print(f\"\\nüìä COLUMNS REMOVED ({len(removed_columns)}):\")\n",
    "if removed_columns:\n",
    "    for col in removed_columns:\n",
    "        print(f\"  ‚ùå {col}\")\n",
    "else:\n",
    "    print(\"  (none)\")\n",
    "\n",
    "print(f\"\\nüìä COLUMNS ADDED ({len(added_columns)}):\")\n",
    "if added_columns:\n",
    "    for col in added_columns:\n",
    "        print(f\"  ‚úÖ {col}\")\n",
    "else:\n",
    "    print(\"  (none)\")\n",
    "\n",
    "print(f\"\\nüìä NET CHANGE: {len(final_columns)} - {len(original_columns)} = {len(final_columns) - len(original_columns):+d} columns\")\n",
    "print(\"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17612787",
   "metadata": {},
   "source": [
    "## 4. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f502ed06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding target labels...\n",
      "\n",
      "Label mapping:\n",
      "  Benign: 0\n",
      "  Bot: 1\n",
      "\n",
      "Binary distribution (0=Benign, 1=Attack):\n",
      "label\n",
      "0    278864\n",
      "1     10935\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Encode target labels\n",
    "print(\"Encoding target labels...\")\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "print(f\"\\nLabel mapping:\")\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    print(f\"  {label}: {i}\")\n",
    "\n",
    "# Convert to binary if needed (benign vs attack)\n",
    "y_binary = (y != 'Benign').astype(int)\n",
    "print(f\"\\nBinary distribution (0=Benign, 1=Attack):\")\n",
    "print(pd.Series(y_binary).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc10e7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into train/test sets...\n",
      "Training set: (231839, 334)\n",
      "Test set: (57960, 334)\n",
      "\n",
      "Train class distribution:\n",
      "label\n",
      "0    223091\n",
      "1      8748\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test class distribution:\n",
      "label\n",
      "0    55773\n",
      "1     2187\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Split data before scaling to prevent data leakage\n",
    "print(\"Splitting data into train/test sets...\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_binary, test_size=0.2, random_state=42, stratify=y_binary\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"\\nTrain class distribution:\\n{pd.Series(y_train).value_counts()}\")\n",
    "print(f\"\\nTest class distribution:\\n{pd.Series(y_test).value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87ab8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "IDENTIFYING FEATURES TO SCALE\n",
      "================================================================================\n",
      "Features TO SCALE (continuous): 304\n",
      "Features NOT to scale (binary/categorical): 30\n",
      "\n",
      "Sample continuous features to scale:\n",
      "  - duration\n",
      "  - packets_count\n",
      "  - fwd_packets_count\n",
      "  - bwd_packets_count\n",
      "  - total_payload_bytes\n",
      "  - fwd_total_payload_bytes\n",
      "  - bwd_total_payload_bytes\n",
      "  - payload_bytes_max\n",
      "  - payload_bytes_min\n",
      "  - payload_bytes_mean\n",
      "\n",
      "Binary/categorical features (keeping as 0/1):\n",
      "  - urg_flag_counts\n",
      "  - rst_flag_counts\n",
      "  - fwd_urg_flag_counts\n",
      "  - fwd_rst_flag_counts\n",
      "  - bwd_urg_flag_counts\n",
      "  - bwd_rst_flag_counts\n",
      "  - urg_flag_percentage_in_total\n",
      "  - fwd_urg_flag_percentage_in_total\n",
      "  - bwd_urg_flag_percentage_in_total\n",
      "  - fwd_urg_flag_percentage_in_fwd_packets\n",
      "  - bwd_urg_flag_percentage_in_bwd_packets\n",
      "  - dst_port_http\n",
      "  - dst_port_https\n",
      "  - dst_port_ssh\n",
      "  - dst_port_ftp\n",
      "  ... and 15 more\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "APPLYING STANDARDSCALER\n",
      "================================================================================\n",
      "Scaling 304 continuous features...\n",
      "‚úì Scaling complete\n",
      "‚úì 30 binary/categorical features kept as 0/1\n",
      "\n",
      "Scaled training set: (231839, 334)\n",
      "Scaled test set: (57960, 334)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "VERIFICATION: Sample of scaled vs unscaled features\n",
      "================================================================================\n",
      "\n",
      "Continuous feature 'duration':\n",
      "  Original range: [0.000, 25444.168]\n",
      "  Scaled range: [-0.156, 133.878]\n",
      "  Scaled mean: -0.000 (should be ~0)\n",
      "  Scaled std: 1.000 (should be ~1)\n",
      "\n",
      "Binary feature 'urg_flag_counts':\n",
      "  Original values: [0]\n",
      "  Scaled values: [0]\n",
      "  ‚úì Binary features remain unchanged!\n",
      "================================================================================\n",
      "\n",
      "Sample of scaled features (first 5 rows, first 10 columns):\n",
      "        duration  packets_count  fwd_packets_count  bwd_packets_count  \\\n",
      "244091 -0.153053      -0.017458          -0.019853          -0.015666   \n",
      "276234 -0.122902      -0.004508          -0.007844          -0.002562   \n",
      "44106   0.159728      -0.016019          -0.015850          -0.015666   \n",
      "214029 -0.143853      -0.008825          -0.007844          -0.009114   \n",
      "177534 -0.127762      -0.007386          -0.007844          -0.006930   \n",
      "\n",
      "        total_payload_bytes  fwd_total_payload_bytes  bwd_total_payload_bytes  \\\n",
      "244091            -0.012254                -0.049846                -0.011491   \n",
      "276234            -0.008791                 0.025153                -0.009194   \n",
      "44106             -0.012582                -0.076556                -0.011402   \n",
      "214029            -0.009333                 0.015863                -0.009592   \n",
      "177534            -0.007253                -0.052363                -0.006445   \n",
      "\n",
      "        payload_bytes_max  payload_bytes_min  payload_bytes_mean  \n",
      "244091          -0.642664          -0.014319           -0.266568  \n",
      "276234           0.602281          -0.014319            0.403869  \n",
      "44106           -0.532993          -0.014319           -0.457968  \n",
      "214029           0.573357          -0.014319            0.456431  \n",
      "177534           0.948166          -0.014319            0.979055  \n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SMART FEATURE SCALING\n",
    "# ============================================================\n",
    "# Don't scale binary/categorical features (already 0/1)\n",
    "# Only scale continuous features\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"IDENTIFYING FEATURES TO SCALE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Automatically detect binary/one-hot features by checking their values\n",
    "# Binary features only contain 0 and 1 (and possibly NaN)\n",
    "no_scale_features = []\n",
    "for col in X_train.columns:\n",
    "    unique_values = X_train[col].dropna().unique()\n",
    "    # Check if column only contains 0 and 1 (binary/one-hot encoded)\n",
    "    if len(unique_values) <= 2 and set(unique_values).issubset({0, 1, 0.0, 1.0}):\n",
    "        no_scale_features.append(col)\n",
    "\n",
    "# Features TO scale (continuous variables)\n",
    "scale_features = [col for col in X_train.columns if col not in no_scale_features]\n",
    "\n",
    "print(f\"Features TO SCALE (continuous): {len(scale_features)}\")\n",
    "print(f\"Features NOT to scale (binary/categorical): {len(no_scale_features)}\")\n",
    "\n",
    "if len(scale_features) > 0:\n",
    "    print(f\"\\nSample continuous features to scale:\")\n",
    "    for feat in scale_features[:10]:\n",
    "        print(f\"  - {feat}\")\n",
    "\n",
    "if len(no_scale_features) > 0:\n",
    "    print(f\"\\nBinary/categorical features (keeping as 0/1):\")\n",
    "    for feat in no_scale_features[:15]:\n",
    "        print(f\"  - {feat}\")\n",
    "    if len(no_scale_features) > 15:\n",
    "        print(f\"  ... and {len(no_scale_features) - 15} more\")\n",
    "\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# ============================================================\n",
    "print(\"=\"*80)\n",
    "print(\"APPLYING STANDARDSCALER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Create copies to avoid modifying original\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "# Scale ONLY the continuous features\n",
    "if len(scale_features) > 0:\n",
    "    print(f\"Scaling {len(scale_features)} continuous features...\")\n",
    "    X_train_scaled[scale_features] = scaler.fit_transform(X_train[scale_features])\n",
    "    X_test_scaled[scale_features] = scaler.transform(X_test[scale_features])\n",
    "    print(\"‚úì Scaling complete\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No continuous features to scale\")\n",
    "\n",
    "# Binary features remain unchanged (already 0/1)\n",
    "if len(no_scale_features) > 0:\n",
    "    print(f\"‚úì {len(no_scale_features)} binary/categorical features kept as 0/1\")\n",
    "\n",
    "print(f\"\\nScaled training set: {X_train_scaled.shape}\")\n",
    "print(f\"Scaled test set: {X_test_scaled.shape}\")\n",
    "\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# ============================================================\n",
    "print(\"=\"*80)\n",
    "print(\"VERIFICATION: Sample of scaled vs unscaled features\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show example of a scaled continuous feature\n",
    "if len(scale_features) > 0:\n",
    "    example_continuous = scale_features[0]\n",
    "    print(f\"\\nContinuous feature '{example_continuous}':\")\n",
    "    print(f\"  Original range: [{X_train[example_continuous].min():.3f}, {X_train[example_continuous].max():.3f}]\")\n",
    "    print(f\"  Scaled range: [{X_train_scaled[example_continuous].min():.3f}, {X_train_scaled[example_continuous].max():.3f}]\")\n",
    "    print(f\"  Scaled mean: {X_train_scaled[example_continuous].mean():.3f} (should be ~0)\")\n",
    "    print(f\"  Scaled std: {X_train_scaled[example_continuous].std():.3f} (should be ~1)\")\n",
    "\n",
    "# Show example of a binary feature\n",
    "if len(no_scale_features) > 0:\n",
    "    example_binary = no_scale_features[0]\n",
    "    print(f\"\\nBinary feature '{example_binary}':\")\n",
    "    print(f\"  Original values: {X_train[example_binary].unique()}\")\n",
    "    print(f\"  Scaled values: {X_train_scaled[example_binary].unique()}\")\n",
    "    print(f\"  ‚úì Binary features remain unchanged!\")\n",
    "\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Show sample of final scaled data\n",
    "print(\"Sample of scaled features (first 5 rows, first 10 columns):\")\n",
    "print(X_train_scaled.iloc[:5, :10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4e6873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "VALIDATING ENGINEERED FEATURES\n",
      "================================================================================\n",
      "\n",
      "‚úì NaN values: 0\n",
      "‚úì Infinite values: 0\n",
      "‚úì All features are valid!\n",
      "\n",
      "================================================================================\n",
      "SAMPLE FEATURE DISTRIBUTIONS\n",
      "================================================================================\n",
      "\n",
      "Port feature distributions:\n",
      "\n",
      "dst_port_http:\n",
      "dst_port_http\n",
      "0    198839\n",
      "1     33000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "dst_port_https:\n",
      "dst_port_https\n",
      "0    180625\n",
      "1     51214\n",
      "Name: count, dtype: int64\n",
      "\n",
      "dst_port_ssh:\n",
      "dst_port_ssh\n",
      "0    230976\n",
      "1       863\n",
      "Name: count, dtype: int64\n",
      "\n",
      "================================================================================\n",
      "‚úì FEATURE ENGINEERING COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "Final dataset ready for training:\n",
      "  Training samples: 231,839\n",
      "  Test samples: 57,960\n",
      "  Total features: 334\n",
      "  Class distribution (train): {0: 223091, 1: 8748}\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FEATURE VALIDATION\n",
    "# ============================================================\n",
    "# Quick sanity checks on the engineered features\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"VALIDATING ENGINEERED FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check for any NaN or infinite values\n",
    "nan_count = X_train_scaled.isna().sum().sum()\n",
    "inf_count = np.isinf(X_train_scaled.select_dtypes(include=[np.number])).sum().sum()\n",
    "\n",
    "print(f\"\\n‚úì NaN values: {nan_count}\")\n",
    "print(f\"‚úì Infinite values: {inf_count}\")\n",
    "\n",
    "if nan_count > 0 or inf_count > 0:\n",
    "    print(\"‚ö†Ô∏è  WARNING: Found NaN or infinite values!\")\n",
    "else:\n",
    "    print(\"‚úì All features are valid!\")\n",
    "\n",
    "# Show distribution of some key features\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE FEATURE DISTRIBUTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Port features\n",
    "port_features = [col for col in X_train_scaled.columns if 'dst_port' in col.lower() and any(s in col for s in ['http', 'ssh', 'https'])]\n",
    "if port_features:\n",
    "    print(f\"\\nPort feature distributions:\")\n",
    "    for feat in port_features[:3]:\n",
    "        print(f\"\\n{feat}:\")\n",
    "        print(X_train_scaled[feat].value_counts())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úì FEATURE ENGINEERING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nFinal dataset ready for training:\")\n",
    "print(f\"  Training samples: {X_train_scaled.shape[0]:,}\")\n",
    "print(f\"  Test samples: {X_test_scaled.shape[0]:,}\")\n",
    "print(f\"  Total features: {X_train_scaled.shape[1]}\")\n",
    "print(f\"  Class distribution (train): {pd.Series(y_train).value_counts().to_dict()}\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a380f3",
   "metadata": {},
   "source": [
    "## 5. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6fa309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving processed data...\n",
      "\n",
      "Processed data saved to: /Users/matthewweaver/Repositories/nidstream/data/processed\n",
      "Files created:\n",
      "  - X_train.csv\n",
      "  - X_test.csv\n",
      "  - y_train.csv\n",
      "  - y_test.csv\n",
      "  - scaler.pkl\n",
      "  - label_encoder.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save processed data\n",
    "print(\"Saving processed data...\")\n",
    "\n",
    "processed_dir = project_root / 'data' / 'processed'\n",
    "processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save train/test splits\n",
    "X_train_scaled.to_csv(processed_dir / 'X_train.csv', index=False)\n",
    "X_test_scaled.to_csv(processed_dir / 'X_test.csv', index=False)\n",
    "pd.Series(y_train, name='label').to_csv(processed_dir / 'y_train.csv', index=False)\n",
    "pd.Series(y_test, name='label').to_csv(processed_dir / 'y_test.csv', index=False)\n",
    "\n",
    "# Save scaler and label encoder for later use\n",
    "import joblib\n",
    "joblib.dump(scaler, processed_dir / 'scaler.pkl')\n",
    "joblib.dump(label_encoder, processed_dir / 'label_encoder.pkl')\n",
    "\n",
    "print(f\"\\nProcessed data saved to: {processed_dir}\")\n",
    "print(\"Files created:\")\n",
    "print(\"  - X_train.csv\")\n",
    "print(\"  - X_test.csv\")\n",
    "print(\"  - y_train.csv\")\n",
    "print(\"  - y_test.csv\")\n",
    "print(\"  - scaler.pkl\")\n",
    "print(\"  - label_encoder.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0193ce",
   "metadata": {},
   "source": [
    "## 6. Prepare SMOTE Data\n",
    "\n",
    "Apply SMOTE to balance classes for model training comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3800109d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PREPARING SMOTE DATA\n",
      "================================================================================\n",
      "\n",
      "Before SMOTE:\n",
      "  Training samples: 231,839\n",
      "  Benign: 223,091\n",
      "  Attack: 8,748\n",
      "  Imbalance ratio: 25.50:1\n",
      "\n",
      "After SMOTE:\n",
      "  Training samples: 446,182\n",
      "  Benign: 223,091\n",
      "  Attack: 223,091\n",
      "  ‚úÖ Classes balanced 1:1\n",
      "\n",
      "‚úÖ SMOTE data saved:\n",
      "  - X_train_smote.csv\n",
      "  - y_train_smote.csv\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Apply SMOTE to create balanced training data\n",
    "print(\"=\"*80)\n",
    "print(\"PREPARING SMOTE DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "print(f\"\\nBefore SMOTE:\")\n",
    "print(f\"  Training samples: {X_train_scaled.shape[0]:,}\")\n",
    "print(f\"  Benign: {(y_train == 0).sum():,}\")\n",
    "print(f\"  Attack: {(y_train == 1).sum():,}\")\n",
    "print(f\"  Imbalance ratio: {(y_train == 0).sum()/(y_train == 1).sum():.2f}:1\")\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\nAfter SMOTE:\")\n",
    "print(f\"  Training samples: {X_train_smote.shape[0]:,}\")\n",
    "print(f\"  Benign: {(y_train_smote == 0).sum():,}\")\n",
    "print(f\"  Attack: {(y_train_smote == 1).sum():,}\")\n",
    "print(f\"  ‚úÖ Classes balanced 1:1\")\n",
    "\n",
    "# Save SMOTE data\n",
    "X_train_smote.to_csv(processed_dir / 'X_train_smote.csv', index=False)\n",
    "pd.Series(y_train_smote, name='label').to_csv(processed_dir / 'y_train_smote.csv', index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ SMOTE data saved:\")\n",
    "print(f\"  - X_train_smote.csv\")\n",
    "print(f\"  - y_train_smote.csv\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12999b7",
   "metadata": {},
   "source": [
    "## 7. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73612f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FEATURE ENGINEERING SUMMARY\n",
      "============================================================\n",
      "\n",
      "Original dataset: 289,799 samples, 323 features\n",
      "Final feature count: 334\n",
      "\n",
      "Training set: 231,839 samples\n",
      "Test set: 57,960 samples\n",
      "\n",
      "Class distribution (train):\n",
      "  Benign: 223,091 (96.2%)\n",
      "  Attack: 8,748 (3.8%)\n",
      "\n",
      "Data ready for model training!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Summary of feature engineering process\n",
    "print(\"=\" * 60)\n",
    "print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nOriginal dataset: {len(df):,} samples, {len(df.columns)} features\")\n",
    "print(f\"Final feature count: {X_train_scaled.shape[1]}\")\n",
    "print(f\"\\nTraining set: {len(X_train_scaled):,} samples\")\n",
    "print(f\"Test set: {len(X_test_scaled):,} samples\")\n",
    "print(f\"\\nClass distribution (train):\")\n",
    "print(f\"  Benign: {(y_train == 0).sum():,} ({(y_train == 0).sum() / len(y_train) * 100:.1f}%)\")\n",
    "print(f\"  Attack: {(y_train == 1).sum():,} ({(y_train == 1).sum() / len(y_train) * 100:.1f}%)\")\n",
    "print(f\"\\nData ready for model training!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c231c7",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "The processed data is now ready for:\n",
    "1. Model training\n",
    "2. Hyperparameter tuning\n",
    "3. Model evaluation and comparison\n",
    "\n",
    "**Note:** You may want to:\n",
    "- Perform feature selection to reduce dimensionality\n",
    "- Experiment with different scaling methods\n",
    "- Create more domain-specific features based on network traffic analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nidstream",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
