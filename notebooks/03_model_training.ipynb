{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7aac49e",
   "metadata": {},
   "source": [
    "# Model Training for Network Intrusion Detection\n",
    "\n",
    "This notebook trains machine learning models on the processed BCCC-CSE-CIC-IDS2018 dataset.\n",
    "\n",
    "## Objectives:\n",
    "1. Load processed features from feature engineering pipeline\n",
    "2. Train baseline models (Logistic Regression, Random Forest, XGBoost)\n",
    "3. Handle class imbalance with SMOTE\n",
    "4. Evaluate model performance\n",
    "5. Compare models using multiple metrics\n",
    "6. Save best model for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79e04f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b79c4e",
   "metadata": {},
   "source": [
    "## 1. Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a71780a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed data...\n",
      "Training set: (190960, 317)\n",
      "Test set: (47740, 317)\n",
      "\n",
      "Class distribution (train):\n",
      "  Benign (0): 182211 (95.4%)\n",
      "  Attack (1): 8749 (4.6%)\n"
     ]
    }
   ],
   "source": [
    "# Load processed data\n",
    "project_root = Path().resolve()\n",
    "processed_dir = project_root / 'data' / 'processed'\n",
    "\n",
    "print(\"Loading processed data...\")\n",
    "X_train = pd.read_csv(processed_dir / 'X_train.csv')\n",
    "X_test = pd.read_csv(processed_dir / 'X_test.csv')\n",
    "y_train = pd.read_csv(processed_dir / 'y_train.csv')['label'].values\n",
    "y_test = pd.read_csv(processed_dir / 'y_test.csv')['label'].values\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"\\nClass distribution (train):\")\n",
    "print(f\"  Benign (0): {(y_train == 0).sum()} ({(y_train == 0).sum() / len(y_train) * 100:.1f}%)\")\n",
    "print(f\"  Attack (1): {(y_train == 1).sum()} ({(y_train == 1).sum() / len(y_train) * 100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6312efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded preprocessing artifacts:\n",
      "  - Scaler: StandardScaler\n",
      "  - Label Encoder: LabelEncoder\n",
      "  - Label classes: ['Benign' 'Bot' nan]\n"
     ]
    }
   ],
   "source": [
    "# Load scaler and label encoder for reference\n",
    "scaler = joblib.load(processed_dir / 'scaler.pkl')\n",
    "label_encoder = joblib.load(processed_dir / 'label_encoder.pkl')\n",
    "\n",
    "print(\"Loaded preprocessing artifacts:\")\n",
    "print(f\"  - Scaler: {type(scaler).__name__}\")\n",
    "print(f\"  - Label Encoder: {type(label_encoder).__name__}\")\n",
    "print(f\"  - Label classes: {label_encoder.classes_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c724696a",
   "metadata": {},
   "source": [
    "## 2. Handle Class Imbalance with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb2967c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying SMOTE to balance classes...\n",
      "Before SMOTE: (190960, 317)\n",
      "  Benign: 182211\n",
      "  Attack: 8749\n",
      "\n",
      "After SMOTE: (364422, 317)\n",
      "  Benign: 182211\n",
      "  Attack: 182211\n",
      "\n",
      "Classes are now balanced!\n"
     ]
    }
   ],
   "source": [
    "# Apply SMOTE to balance classes\n",
    "print(\"Applying SMOTE to balance classes...\")\n",
    "print(f\"Before SMOTE: {X_train.shape}\")\n",
    "print(f\"  Benign: {(y_train == 0).sum()}\")\n",
    "print(f\"  Attack: {(y_train == 1).sum()}\")\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"\\nAfter SMOTE: {X_train_balanced.shape}\")\n",
    "print(f\"  Benign: {(y_train_balanced == 0).sum()}\")\n",
    "print(f\"  Attack: {(y_train_balanced == 1).sum()}\")\n",
    "print(f\"\\nClasses are now balanced!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d019d953",
   "metadata": {},
   "source": [
    "## 3. Train Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5ec8bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store trained models and results\n",
    "models = {}\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f008670d",
   "metadata": {},
   "source": [
    "### 3.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6854a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression...\n",
      "Training completed in 23.29 seconds\n",
      "\n",
      "Logistic Regression Results:\n",
      "  accuracy: 0.9999\n",
      "  precision: 0.9973\n",
      "  recall: 1.0000\n",
      "  f1: 0.9986\n",
      "  roc_auc: 1.0000\n",
      "  train_time: 23.29s\n"
     ]
    }
   ],
   "source": [
    "# Train Logistic Regression\n",
    "print(\"Training Logistic Regression...\")\n",
    "start_time = time.time()\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)\n",
    "lr_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "print(f\"Training completed in {train_time:.2f} seconds\")\n",
    "\n",
    "# Predictions\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "y_pred_proba_lr = lr_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Store model\n",
    "models['Logistic Regression'] = lr_model\n",
    "\n",
    "# Evaluate\n",
    "results['Logistic Regression'] = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_lr),\n",
    "    'precision': precision_score(y_test, y_pred_lr),\n",
    "    'recall': recall_score(y_test, y_pred_lr),\n",
    "    'f1': f1_score(y_test, y_pred_lr),\n",
    "    'roc_auc': roc_auc_score(y_test, y_pred_proba_lr),\n",
    "    'train_time': train_time\n",
    "}\n",
    "\n",
    "print(\"\\nLogistic Regression Results:\")\n",
    "for metric, value in results['Logistic Regression'].items():\n",
    "    if metric != 'train_time':\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {metric}: {value:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3275266e",
   "metadata": {},
   "source": [
    "### 3.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b58ca2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest...\n",
      "Training completed in 15.79 seconds\n",
      "\n",
      "Random Forest Results:\n",
      "  accuracy: 0.9925\n",
      "  precision: 0.8590\n",
      "  recall: 1.0000\n",
      "  f1: 0.9241\n",
      "  roc_auc: 1.0000\n",
      "  train_time: 15.79s\n"
     ]
    }
   ],
   "source": [
    "# Train Random Forest\n",
    "print(\"Training Random Forest...\")\n",
    "start_time = time.time()\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "print(f\"Training completed in {train_time:.2f} seconds\")\n",
    "\n",
    "# Predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "y_pred_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Store model\n",
    "models['Random Forest'] = rf_model\n",
    "\n",
    "# Evaluate\n",
    "results['Random Forest'] = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_rf),\n",
    "    'precision': precision_score(y_test, y_pred_rf),\n",
    "    'recall': recall_score(y_test, y_pred_rf),\n",
    "    'f1': f1_score(y_test, y_pred_rf),\n",
    "    'roc_auc': roc_auc_score(y_test, y_pred_proba_rf),\n",
    "    'train_time': train_time\n",
    "}\n",
    "\n",
    "print(\"\\nRandom Forest Results:\")\n",
    "for metric, value in results['Random Forest'].items():\n",
    "    if metric != 'train_time':\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {metric}: {value:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0101598d",
   "metadata": {},
   "source": [
    "### 3.3 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766062b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost...\n"
     ]
    }
   ],
   "source": [
    "# Train XGBoost\n",
    "print(\"Training XGBoost...\")\n",
    "start_time = time.time()\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "xgb_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "print(f\"Training completed in {train_time:.2f} seconds\")\n",
    "\n",
    "# Predictions\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "y_pred_proba_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Store model\n",
    "models['XGBoost'] = xgb_model\n",
    "\n",
    "# Evaluate\n",
    "results['XGBoost'] = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_xgb),\n",
    "    'precision': precision_score(y_test, y_pred_xgb),\n",
    "    'recall': recall_score(y_test, y_pred_xgb),\n",
    "    'f1': f1_score(y_test, y_pred_xgb),\n",
    "    'roc_auc': roc_auc_score(y_test, y_pred_proba_xgb),\n",
    "    'train_time': train_time\n",
    "}\n",
    "\n",
    "print(\"\\nXGBoost Results:\")\n",
    "for metric, value in results['XGBoost'].items():\n",
    "    if metric != 'train_time':\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {metric}: {value:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d1cb6c",
   "metadata": {},
   "source": [
    "## 4. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab88b9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string())\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6192cd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    values = [results[model][metric] for model in results.keys()]\n",
    "    bars = ax.bar(results.keys(), values, color=['steelblue', 'orange', 'green'][:len(results)])\n",
    "    \n",
    "    ax.set_ylabel(metric.capitalize())\n",
    "    ax.set_title(f'{metric.capitalize()} Comparison')\n",
    "    ax.set_ylim([0, 1.05])\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}',\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375f8389",
   "metadata": {},
   "source": [
    "## 5. Detailed Evaluation - Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e69681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify best model based on F1 score\n",
    "best_model_name = max(results, key=lambda x: results[x]['f1'])\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"F1 Score: {results[best_model_name]['f1']:.4f}\")\n",
    "\n",
    "# Get predictions from best model\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "y_pred_proba_best = best_model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0579007c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Benign', 'Attack'],\n",
    "            yticklabels=['Benign', 'Attack'])\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nConfusion Matrix Values:\")\n",
    "print(f\"True Negatives (TN): {cm[0, 0]}\")\n",
    "print(f\"False Positives (FP): {cm[0, 1]}\")\n",
    "print(f\"False Negatives (FN): {cm[1, 0]}\")\n",
    "print(f\"True Positives (TP): {cm[1, 1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f718e4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_best, target_names=['Benign', 'Attack']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af917a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_best)\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba_best)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, linewidth=2, label=f'{best_model_name} (AUC = {auc_score:.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5b73ef",
   "metadata": {},
   "source": [
    "## 6. Feature Importance (for tree-based models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7f92a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for Random Forest or XGBoost\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Plot top 20 features\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    top_features = feature_importance.head(20)\n",
    "    plt.barh(range(len(top_features)), top_features['importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title(f'Top 20 Feature Importances - {best_model_name}')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    print(feature_importance.head(10).to_string(index=False))\n",
    "else:\n",
    "    print(f\"{best_model_name} does not support feature importances.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b85d6e",
   "metadata": {},
   "source": [
    "## 7. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3399441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model\n",
    "models_dir = project_root / 'models'\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_path = models_dir / f'best_model_{best_model_name.lower().replace(\" \", \"_\")}.pkl'\n",
    "joblib.dump(best_model, model_path)\n",
    "\n",
    "print(f\"\\nBest model saved to: {model_path}\")\n",
    "\n",
    "# Save model metadata\n",
    "metadata = {\n",
    "    'model_name': best_model_name,\n",
    "    'model_type': type(best_model).__name__,\n",
    "    'metrics': results[best_model_name],\n",
    "    'training_samples': len(X_train_balanced),\n",
    "    'features': list(X_train.columns),\n",
    "    'n_features': len(X_train.columns)\n",
    "}\n",
    "\n",
    "metadata_path = models_dir / 'model_metadata.pkl'\n",
    "joblib.dump(metadata, metadata_path)\n",
    "\n",
    "print(f\"Model metadata saved to: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1db5a2d",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6922c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Training samples (after SMOTE): {len(X_train_balanced):,}\")\n",
    "print(f\"  Test samples: {len(X_test):,}\")\n",
    "print(f\"  Number of features: {X_train.shape[1]}\")\n",
    "print(f\"\\nModels Trained: {len(models)}\")\n",
    "for model_name in models.keys():\n",
    "    print(f\"  - {model_name}\")\n",
    "print(f\"\\nBest Model: {best_model_name}\")\n",
    "print(f\"  Accuracy: {results[best_model_name]['accuracy']:.4f}\")\n",
    "print(f\"  Precision: {results[best_model_name]['precision']:.4f}\")\n",
    "print(f\"  Recall: {results[best_model_name]['recall']:.4f}\")\n",
    "print(f\"  F1 Score: {results[best_model_name]['f1']:.4f}\")\n",
    "print(f\"  ROC-AUC: {results[best_model_name]['roc_auc']:.4f}\")\n",
    "print(f\"\\nModel saved to: {model_path}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f6937b",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Hyperparameter Tuning**: Use GridSearchCV or Optuna to find optimal hyperparameters\n",
    "2. **Feature Selection**: Try reducing features to improve model performance and speed\n",
    "3. **Ensemble Methods**: Combine multiple models for better predictions\n",
    "4. **Cross-Validation**: Perform k-fold cross-validation for more robust evaluation\n",
    "5. **Deployment**: Use the saved model in the inference pipeline (`src/inference_pipeline/`)\n",
    "6. **API Integration**: Integrate with FastAPI service (`src/api/main.py`)\n",
    "7. **Dashboard**: Visualize predictions in Streamlit dashboard (`app.py`)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nidstream",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
