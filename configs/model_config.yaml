# Model configuration
model:
  type: xgboost  # Options: xgboost, isolation_forest, autoencoder
  parameters:
    max_depth: 6
    learning_rate: 0.1
    n_estimators: 100
    subsample: 0.8
    colsample_bytree: 0.8
    gamma: 0
    min_child_weight: 1
    reg_alpha: 0
    reg_lambda: 1
    objective: binary:logistic
    eval_metric: aucpr
    random_state: 42

# Training configuration
training:
  batch_size: 10000
  early_stopping_rounds: 10
  use_gpu: false
  n_jobs: -1

# Feature engineering
features:
  temporal_features: true
  ratio_features: true
  statistical_features: true
  protocol_features: true
  log_transform: true
  
  # Features to drop (if any)
  drop_features:
    - Timestamp
    - ts

# Data splits (by date)
data_splits:
  train_end: "2018-02-20"
  val_end: "2018-02-22"
  # test: everything after val_end

# Class imbalance handling
class_balance:
  strategy: scale_pos_weight  # Options: scale_pos_weight, smote, none
  contamination: 0.1  # For Isolation Forest
